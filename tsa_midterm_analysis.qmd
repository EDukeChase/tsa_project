---
title: "Timerseries Analysis Midterm Analysis"
author: "E. Duke Chase"
date: today
---

# Load Packages
```{r}
#| label: setup

# General settings and ancillary packages
source("R/setup.R")
source("R/cache_computation.R")
source("R/export_controls.R")
source("R/save_plot.R")

# Project libraries
library(forecast)
library(tseries)

# Default color palette
nav_cols <- brewer.pal(8, "Set2")

# Change default target directory, file formats, and default save behavior
# Refer to `export_controls.R` for mode options
ctrl <- export_controls(mode = "export_backup")

# Wrap save_plot with project defaults + export policy
save_plot <- apply_save_plot_defaults(
  save_plot_fun   = save_plot,
  ctrl            = ctrl,
  default_target  = "output/figures",
  default_format  = c("png", "svg")
)
```

# Data Cleaning

```{r}
#| label: load-process

# Load data
ili_raw <- read.csv(here("data", "ILINet.csv"),
                    header = TRUE,
                    skip = 1)

# Clean data
ili_clean <- ili_raw |>
  # Remove rows with missing data or no unique data
  select(
    -REGION.TYPE,         # All are "National"
    -REGION,              # All are "X"
    -X.UNWEIGHTED.ILI,    # Raw, not weighted by state size
    -contains("AGE"),     # Two of the demographic breakdowns are empty
    -ILITOTAL,            # Raw values 
    -NUM..OF.PROVIDERS,   # Raw values
    -TOTAL.PATIENTS       # Raw values
  ) |>
  
  # Standardize variable naming
  rename(year = YEAR, week = WEEK, ili_w = X..WEIGHTED.ILI) |>  
  
  # Modify to work two-year spanning flu-seasons (start week is 40)
  mutate(
    flu_season = if_else(week >= 40, year, year - 1),
    season_week = if_else(week >= 40, week - 39, week + 13),
    season_label = paste0(flu_season, "-", flu_season + 1
    )
  ) |> 
  # Remove data from the incomplete 2025-2026 flu season
  filter(flu_season < 2025)
```

One issue I'll have to deal with right off the bat is that some years have 53 weeks and some have 52. For simplicity of analysis I'm going to start off by just dropping the 53rd week from years that have it, but I will go back and do a sensitivity analysis comparing a few methods (imputing 53rd weeks for other years, averaging the 53rd week across the 52nd and 1st of next year, etc.).

```{r}
#| label: week-53-problem

# Create working dataset for seasonality check
ili_seasonal <- ili_clean |>
  # Drop 53rd week to standardize length (naive approach, revisit later)
  filter(week <= 52) |> 
  arrange(year, week)

# Create ts object
ili_ts_raw <- ts(
  ili_seasonal$ili_w,
  start = c(ili_seasonal$year[1], ili_seasonal$week[1]),
  frequency = 52
)
```

```{r}
#| label: fig-raw-data
#| fig-cap: "Weighted ILI Activity"

plot(ili_ts_raw, 
     main = "Weighted ILI Activity", 
     ylab = "Weighted ILI %")
raw_data_rec <- recordPlot()

if (ctrl$outputs) {
  save_plot(raw_data_rec, "raw_data")
}
```


Looking at this plot, it is apparent that until about 2003, data were only collected during flu season and not over the summer months, so those flu seasons will be excluded from the analysis.

```{r}
#| label: tbl-find-zero-weeks

zero_check <- ili_seasonal |> 
  group_by(flu_season) |> 
  summarize(
    weeks_with_zero = sum(ili_w == 0, na.rm = TRUE),
    min_ili = min(ili_w, na.rm = TRUE),
    total_weeks = n()
  )

print(zero_check, n = 8)
```

Based on this output, 2002 - 2003 is the first flu season with no missing weeks, so I will discard all data before that.
Further research showed that there was sporadic collection for the summer of 2003 [@CDCExpandMonitoring], and only data on children for the summers of 2004 and 2005 [@CDCSum03], so the 2005-06 flu season will be the first included in the dataset.

```{r}
#| label: drop-pre-2005-season

# Drop data before 2005 flu season
ili_df <- ili_seasonal |> 
  filter(flu_season >= 2005) |> 
  arrange(year, week)

# Overwrite previous ts object
ili_ts <- ts(
  ili_df$ili_w,
  start = c(2005, 40),
  frequency = 52
)
```

```{r}
#| label: fig-clean-data
#| fig-cap: "Clean ILI Activity"

plot(ili_ts, 
     main = "Weighted ILI Activity (Cleaned)", 
     ylab = "Weighted ILI %")
clean_data_rec <- recordPlot()

if (ctrl$outputs) {
  save_plot(clean_data_rec, "clean_data")
}
```


# Exploratory Analysis
```{r}
#| label: build-flu-overview-plot

highlight_years <- c("2009-2010", "2011-2012", "2017-2018", "2019-2020", 
                     "2020-2021", "2022-2023", "2024-2025")
                     
mini_colors <- brewer.pal(7, "Dark2")
mini_tags <- paste0("(", letters[1:7],")")
                     
p_main <- ggplot(ili_df, aes(x = season_week, y = ili_w, group = flu_season)) +
  geom_line(color = "grey60", alpha = 0.5, size = 0.5) +
  labs(
    x = "Weeks into Flu Season",
    y = "Weighted ILI%"
  )

create_mini_plot <- function(i) {
  
  target_year <- highlight_years[i]
  target_color <- mini_colors[i]
  target_tag <- mini_tags[i]
  
  mini_df <- ili_df |> 
    mutate(
      is_focus = if_else(season_label == target_year, "Focus", "Background")
    ) |> 
    arrange(is_focus)
  
  ggplot(mini_df, aes(x = season_week, y = ili_w, group = flu_season)) +
    geom_line(aes(color = is_focus, size = is_focus, alpha = is_focus)) +
    
    scale_color_manual(values = c("Background" = "grey90", "Focus" = target_color)) +
    scale_size_manual(values = c("Background" = 0.3, "Focus" = 1.0)) +
    scale_alpha_manual(values = c("Background" = 0.7, "Focus" = 1.0)) +
    
    theme_void() + 
    theme(
      axis.title.x = element_text(size = 9, face = "bold", margin = margin(t = 2, b = 10)),
      panel.border = element_rect(color = "grey90", fill = NA),
      plot.tag = element_text(size = 10, face = "bold"),
      plot.tag.position = c(0.95, 0.95)
    ) +
    labs(
      x = target_year,
      tag = target_tag
    ) + 
    guides(color = "none", size = "none", alpha = "none")
}

mini_plots <- lapply(1:7, create_mini_plot)

p1 <- mini_plots[[1]]
p2 <- mini_plots[[2]]
p3 <- mini_plots[[3]]
p4 <- mini_plots[[4]]
p5 <- mini_plots[[5]]
p6 <- mini_plots[[6]]
p7 <- mini_plots[[7]]

layout_design <- "
AAAB
AAAC
AAAD
EFGH
"

flu_overview <- p_main + p1 + p2 + p3 + p4 + p5 + p6 + p7 +
  plot_layout(design = layout_design) +
  plot_annotation(
    title = "Weighted ILI %: Historical Overview & Key Anomalies",
    subtitle = "Main plot (Left) shows the full historical range.\nSubplots (Right/Bottom) isolate notable seasons."
    )
```

```{r}
#| label: fig-flu-overview
#| fig-cap: "Weighted ILI%: Historical overview and selected anomalous seasons."
#| fig-asp: 0.714

flu_overview

if (ctrl$outputs) {
  save_plot(flu_overview, "flu_overview")
}
```

In the main figure we can see that for most flu seasons there's a peak between 13-18 weeks into the season, but there is a reasonable amount of variation in this, with one peak happening as early as 3 weeks into the flu season, and another as late as 25 weeks into the season. This is definitely more variation than would be seen just due to deleting week 53 or the slight variation in how weeks line up year to year. Interestingly, despite the wide distribution of peaks during flu season, the trough of flu season almost universally occurs around week 30.

Around the main plot I've highlighted a few flu seasons that had interesting characteristics. Subplots (a) and (c) were both heavy flu years, with (a) reflecting the H1N1 (swine flu) epidemic; swine flu had a very early start in the year compared to typical flu variants, and is also the only time I've experienced hallucinations due to fever. Subplot (b) was noteworthy due to being an incredibly mild flu season. Subplot (g) was interesting because it had two distinct peaks during the season. Lastly, subplots (d), (e) and (f) are especially indicative of the effects COVID-19 had on flu occurrence. Subplot (d) shows the flu season which included the initial spread of COVID-19, which may explain why it has three peaks instead of one like the majority of seasons have, especially since early COVID-19 strains had significant symptom overlap with influenza, and this data is related to doctor visits, not tested strains. Subplot (e) occurred during the height of non-pharmaceutical interventions intended to slow the spread (masking, social distancing, etc.)[@covid_npi], showing that there was almost no detectable flu season as a result. Subplot (f) occurred soon after the vast majority of state-level interventions had been lifted[@covid_npi], and peaked earlier in the year than typical, possibly lending credence to the idea of immune debt [@immune_debt] causing people to be more vulnerable to infection after a period of low exposure.

At this point of the exploratory analysis, it's worth splitting into the two datasets I'll be using for the analysis., since the characteristics change depending on the time period in question due to just how big of a shock event COVID-19 was to ILI% numbers. 
```{r}
#| label: split_data-by-covid

# Create updated dataframes and timeseries
# Full History [2005 - 2025]
ili_fh <- ili_df                       
ili_fh_ts <- ili_ts

# Pre-Pandemic [2005 - 2019)
ili_pp <- ili_df |>                    
  filter(flu_season < 2019)
ili_pp_ts <- ts(
  ili_pp$ili_w,
  start = c(2005, 40),
  frequency = 52
)

# Pandemic Era [2019 - 2025]
ili_pe <- ili_df |>                    
  filter(flu_season >= 2019)

ili_pe_ts <- ts(
  ili_pe$ili_w,
  start = c(2019, 40),
  frequency = 52
)
```

The full history dataset covers flu seasons 2005-06 through 2024-25 ($\text{N}_{CI} = $`r nrow(ili_fh)`, $\text{S}_{CI} = $`r n_distinct(ili_fh$season_label)`), the pre-pandemic dataset covers flu seasons 2005-06 through 2018-19 ($\text{N}_{CE} = $ `r nrow(ili_pp)`, $\text{S}_{CE} = $`r n_distinct(ili_pp$season_label)`), and the pandemic era dataset covers flu seasons 2019-20 through 2024-25 ($\text{N}_{PE} = $`r nrow(ili_pe)`, $\text{S}_{PE} = $`r n_distinct(ili_pe$season_label)`). The pandemic era dataset is only used to compare the pre- and post-pandemic trends, but will not be analyzed on its own.


```{r}
#| label: build-peak-compare-table

season_peaks_tbl <- function(df) {
  df |>
    group_by(flu_season, season_label) |>
    summarise(
      peak_ili   = max(ili_w, na.rm = TRUE),
      peak_week  = season_week[which.max(ili_w)],
      trough_ili = min(ili_w, na.rm = TRUE),
      amplitude  = peak_ili - trough_ili,
      .groups = "drop"
    )
}

peak_summary_tbl <- function(season_peaks) {
  season_peaks |>
    summarise(
      seasons           = n(),
      peak_week_median  = median(peak_week, na.rm = TRUE),
      peak_week_iqr     = IQR(peak_week, na.rm = TRUE),
      peak_week_min     = min(peak_week, na.rm = TRUE),
      peak_week_max     = max(peak_week, na.rm = TRUE),
      peak_ili_median   = median(peak_ili, na.rm = TRUE),
      peak_ili_iqr      = IQR(peak_ili, na.rm = TRUE),
      amplitude_median  = median(amplitude, na.rm = TRUE),
      amplitude_iqr     = IQR(amplitude, na.rm = TRUE),
      .groups = "drop"
    ) |>
    mutate(
      peak_week_minmax = paste0(peak_week_min, "-", peak_week_max)
    ) |>
    select(
      seasons,
      peak_week_median, peak_week_iqr, peak_week_minmax,
      peak_ili_median, peak_ili_iqr,
      amplitude_median, amplitude_iqr
    )
}

peak_long_tbl <- function(peak_summary) {
  peak_summary |>
    (\(x) enframe(as.list(x), name = "Metric", value = "Value"))() |>
    mutate(
      Metric = recode(
        Metric,
        seasons          = "Number of seasons",
        peak_week_median = "Peak week (median)",
        peak_week_iqr    = "Peak week (IQR)",
        peak_week_minmax = "Peak week (min-max)",
        peak_ili_median  = "Peak ILI% (median)",
        peak_ili_iqr     = "Peak ILI% (IQR)",
        amplitude_median = "Season amplitude (median)",
        amplitude_iqr    = "Season amplitude (IQR)",
        .default = Metric
      ),
      Value = map_chr(Value, \(v) {
        if (is.numeric(v)) sprintf("%.2f", v) else as.character(v)
      })
    )
}

fmt_p_tests <- function(p) {
  if (is.na(p)) return("")
  if (p < 0.001) return("< 0.001")
  sprintf("%.3f", p)
}

# ---- build season-level peaks for each dataset ----
peaks_pp <- season_peaks_tbl(ili_pp)
peaks_pe <- season_peaks_tbl(ili_pe)
peaks_fh <- season_peaks_tbl(ili_fh)

# Welch Test (PP vs PE) on season-level metrics
welch_pvals <- tibble( 
  Metric = c("Peak ILI% (median)", "Season amplitude (median)", "Peak week (median)"), 
  `Welch p (PP vs PE)` = c( 
    fmt_p_tests(t.test(peaks_pp$peak_ili, peaks_pe$peak_ili, alternative = "two.sided")$p.value),
    fmt_p_tests(t.test(peaks_pp$amplitude, peaks_pe$amplitude, alternative = "two.sided")$p.value),
    fmt_p_tests(t.test(peaks_pp$peak_week, peaks_pe$peak_week, alternative = "two.sided")$p.value) 
    ) 
  )

# ---- summaries into long form ----
ce_long <- peak_long_tbl(peak_summary_tbl(peaks_pp)) |> rename(`Pre-Pandemic (2005-2019)` = Value)
co_long <- peak_long_tbl(peak_summary_tbl(peaks_pe)) |> rename(`Pandemic Era (2019-2025)`       = Value)
ci_long <- peak_long_tbl(peak_summary_tbl(peaks_fh)) |> rename(`Full History (2005-2025)`= Value)

# ---- combine ----
peak_compare_tbl <- ce_long |>
  left_join(co_long, by = "Metric") |>
  left_join(welch_pvals, by = "Metric") |>
  left_join(ci_long, by = "Metric") |>
  mutate(`Welch p (PP vs PE)` = replace_na(`Welch p (PP vs PE)`, "")) |>
  rename(
    `Season-level summary` = Metric,
    `P-value` = `Welch p (PP vs PE)`
  )

# ---- build renderable table object ----
peak_compare_kbl <- peak_compare_tbl |>
  kable(
    format = if (is_latex_output()) "latex" else "html",
    booktabs = TRUE,
    align = c("l", "c", "c", "c", "c")
  ) |>
  add_header_above(c(
    " " = 1,
    "Cohort Comparison (Welch t-test)" = 3,
    "Reference" = 1
  )) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: tbl-peak-compare
#| tbl-cap: "Season-level peak timing and intensity summaries."

peak_compare_kbl

if (ctrl$outputs) {
  save_rds_if(
    peak_compare_tbl,
    here("output", "tables", "peak_compare_tbl.rds"),
    ctrl
  )
}
```


```{r}
#| label: build-variability-plot

make_season_profile <- function(df) {
  df |>
    group_by(season_week) |>
    summarise(
      med = median(ili_w, na.rm = TRUE),
      q25 = quantile(ili_w, 0.25, na.rm = TRUE),
      q75 = quantile(ili_w, 0.75, na.rm = TRUE),
      .groups = "drop"
    )
}

make_profile_plot <- function(df, title) {
  prof <- make_season_profile(df)

  ggplot(prof, aes(x = season_week)) +
    geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2) +
    geom_line(aes(y = med), linewidth = 0.8) +
    labs(
      title = title,
      subtitle = "Median ILI% with interquartile range across seasons",
      x = "Weeks into flu season",
      y = "Weighted ILI%"
    ) +
    theme_minimal()
}

p_fh <- make_profile_plot(ili_fh, "Full History (2005-2025)")
p_pp <- make_profile_plot(ili_pp, "Pre-Pandemic (2005-2019)")
p_pe <- make_profile_plot(ili_pe, "Pandemic Era (2019-2025)")

# Use shared scales
y_max <- max(
  make_season_profile(ili_fh)$q75,
  make_season_profile(ili_pp)$q75,
  make_season_profile(ili_pe)$q75,
  na.rm = TRUE
)

p_fh <- p_fh + coord_cartesian(ylim = c(0, y_max))
p_pp <- p_pp + coord_cartesian(ylim = c(0, y_max))
p_pe <- p_pe + coord_cartesian(ylim = c(0, y_max))

layout <- "
#AA#
BBCC
"

peak_var_plot <- (p_fh + p_pp + p_pe) +
  plot_layout(design = layout) +
  plot_annotation(
    title = "Typical flu-season pattern and variability by week",
    subtitle = "Top: Full history (centered). Bottom: Pre- vs Post-COVID-19 comparison."
  )
```

```{r}
#| label: fig-peak-var
#| fig-cap: "Typical flu-season pattern and variability by week"
#| fig-asp: 0.714

peak_var_plot

if (ctrl$outputs) {
  save_plot(peak_var_plot, "peak_var")
}
```

# Analysis Prep
## Train-Test Split
```{r}
#| label: train-test-split

# Testing data with COVID-19 excluded
ili_pp_test <- ili_pp |> 
  filter(season_label == "2018-2019")
test_pp_ts <- ts(
  ili_pp_test$ili_w,
  start = c(ili_pp_test$year[1], ili_pp_test$week[1]),
  frequency = 52
)

# Training data with COVID-19 excluded
ili_pp_train <- ili_pp |> 
  filter(season_label != "2018-2019")
train_pp_ts <- ts(
  ili_pp_train$ili_w,
  start = c(ili_pp_train$year[1], ili_pp_train$week[1]),
  frequency = 52
)

# Testing data with COVID-19 included
ili_fh_test <- ili_fh |> 
  filter(season_label == "2024-2025")
test_fh_ts <- ts(
  ili_fh_test$ili_w,
  start = c(ili_fh_test$year[1], ili_fh_test$week[1]),
  frequency = 52
)

# Training data with COVID-19 included
ili_fh_train <- ili_fh |> 
  filter(season_label != "2024-2025")
train_fh_ts <- ts(
  ili_fh_train$ili_w,
  start = c(ili_fh_train$year[1], ili_fh_train$week[1]),
  frequency = 52
)
```

## Variance Stabilization
```{r}
#| label: boxcox_transform

# Estimate lambda on training data
lambda_pp <- BoxCox.lambda(as.numeric(train_pp_ts), method = "guerrero")
lambda_fh <- BoxCox.lambda(as.numeric(train_fh_ts), method = "guerrero")

# Store the lambdas in a way that they are easily retrievable for back-transformation
bc_params <- tibble(
  dataset = c("pp", "fh"),
  lambda_hat = c(lambda_pp, lambda_fh),
  method = "guerrero"
)
saveRDS(bc_params, here("data", "boxcox_params_train_only.rds"))

# Apply BoxCox transformation to training data
train_pp_bc_ts <- BoxCox(train_pp_ts, lambda_pp)
train_fh_bc_ts <- BoxCox(train_fh_ts, lambda_fh)

# Helper functions for plots
ts_to_df <- function(x) tibble(time = as.numeric(time(x)), value = as.numeric(x))

make_ts_plot <- function(x, title, ylab) {
  ggplot(ts_to_df(x), aes(time, value)) +
    geom_line(linewidth = 0.35) +
    labs(title = title, x = NULL, y = ylab) +
    theme_minimal()
}

# Create the plots
p_pp_raw <- make_ts_plot(train_pp_ts, "Pre-Pandemic: raw", "ILI%")
p_fh_raw <- make_ts_plot(train_fh_ts, "Full History: raw", "ILI%")

p_pp_bc <- make_ts_plot(
  train_pp_bc_ts,
  bquote("Pre-Pandemic: BoxCox (" * hat(lambda) == .(sprintf("%.3f", lambda_pp)) * ")"),
  "BoxCox(ILI%)"
)
p_fh_bc <- make_ts_plot(
  train_fh_bc_ts,
  bquote("Full History: BoxCox (" * hat(lambda) == .(sprintf("%.3f", lambda_fh)) * ")"),
  "BoxCox(ILI%)"
)

data_transformation_plot <- (p_pp_raw | p_fh_raw) /
(p_pp_bc  | p_fh_bc) +
  plot_annotation(
    title = "Variance stabilization on training sets (Box-Cox)",
    subtitle = bquote(hat(lambda) * " estimated with Guerrero method")
  )
```
```{r}
#| label: fig-data-transformation
#| fig-asp: 0.5

data_transformation_plot

if (ctrl$outputs) {
  save_plot(data_transformation_plot, "data_transformation")
}
```


As can be seen, the transformed training datasets visible in the bottom row exhibit much less variability in their peaks over the years. There is a loss of stability in terms of the lowest values, as the summer months were very constant on the original scale, but this is worth the reduction in variability of the peaks.

```{r}
#| label: stl_decomposition
#| fig-width: 12
#| fig-height: 6

stl_pp <- stl(train_pp_bc_ts, s.window = "periodic", robust = TRUE)
stl_fh <- stl(train_fh_bc_ts, s.window = "periodic", robust = TRUE)

p_stl_pp <- autoplot(stl_pp) + ggtitle("STL decomposition\n(PP train, Box-Cox)")
p_stl_fh <- autoplot(stl_fh) + ggtitle("STL decomposition\n(FH train, Box-Cox)")

decomp_plot <- (p_stl_pp | p_stl_fh) +
  plot_annotation(title = "STL Decomposition on Transformed Training Series")
```

```{r}
#| label: fig-decomp-plot
#| fig-cap: "STL Decomposition on Transformed Training Series"

decomp_plot

if (ctrl$outputs) {
  save_plot(decomp_plot, "transformed_decomposition")
}
```


The decompositions for the two datasets both look okay, although obviously not perfect because we'd want a flat trend line and for the remainders to look more like white noise. As it is, the trend line is all over the place, but the white noise looks decent to me when you consider that we are dealing with a variable, biological process. The biggest outliers are around 2009-2010 (H1N1) and 2020-2024 (COVID-19); other than that there are small variations which clearly aren't white noise, but are pretty minimal and reflect slight shifts in when exactly the flu season starts and peaks in a given year.


```{r}
#| label: build-acf-pacf

acf_theme <- theme_bw() +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 11),
    axis.text  = element_text(size = 9)
  )

seasonal_highlight <- list(
  annotate("rect",
    xmin = 50, xmax = 54,
    ymin = -Inf, ymax = Inf,
    fill = brewer.pal(3, "Dark2")[1],
    alpha = 0.2
  ),
  geom_vline(
    xintercept = 52,
    color = brewer.pal(3, "Dark2")[1],
    linetype = "dashed",
    alpha = 0.6
  )
)

make_acf_pacf_set <- function(x_ts, col_label) {
  x <- as.numeric(x_ts)

  acf_st  <- ggAcf(x, lag.max = 26) +
    labs(title = paste0(col_label, ": Short-term ACF"), x = "Lag (weeks)") + acf_theme

  pacf_st <- ggPacf(x, lag.max = 26) +
    labs(title = paste0(col_label, ": Short-term PACF"), x = "Lag (weeks)") + acf_theme

  acf_lt  <- ggAcf(x, lag.max = 104) + seasonal_highlight +
    labs(title = paste0(col_label, ": Long-term ACF (2 years)"), x = "Lag (weeks)") + acf_theme

  pacf_lt <- ggPacf(x, lag.max = 104) + seasonal_highlight +
    labs(title = paste0(col_label, ": Long-term PACF (2 years)"), x = "Lag (weeks)") + acf_theme

  list(acf_st = acf_st, pacf_st = pacf_st, acf_lt = acf_lt, pacf_lt = pacf_lt)
}

# Use your transformed training series here:
pp <- make_acf_pacf_set(train_pp_bc_ts, "Pre-Pandemic")
fh <- make_acf_pacf_set(train_fh_bc_ts, "Full History")

acf_pacf_grid <-
  (pp$acf_st  | fh$acf_st) /
  (pp$pacf_st | fh$pacf_st) /
  (pp$acf_lt  | fh$acf_lt) /
  (pp$pacf_lt | fh$pacf_lt) +
  plot_annotation(
    title = "ACF/PACF Diagnostics on Transformed Training Sets",
    subtitle = "Short-term (≤26 weeks) and long-term (≤104 weeks) structure;\nannual seasonality highlighted near lag 52",
    caption = "Shading marks the annual seasonal cycle region (weeks 50-54)."
  )
```

```{r}
#| label: fig-acf-pacf
#| fig-cap: "ACF/PACF Diagnostics on Transformed Training Sets"

acf_pacf_grid

if (ctrl$outputs) {
  save_plot(acf_pacf_grid, "acf_pacf")
}
```


## Pre-Modeling Checks
```{r}
#| label: build-premodel-checks-table

fmt_num <- function(x, digits = 3) {
  if (is.na(x)) return(NA_character_)
  formatC(x, format = "f", digits = digits)
}

fmt_p_peak <- function(p) {
  if (is.na(p)) return(NA_character_)
  if (p < 0.001) "< 0.001" else fmt_num(p, 3)
}

premodel_checks <- function(x_ts, label) {
  x_ts <- as.ts(x_ts)

  adf_p  <- tryCatch(adf.test(x_ts, alternative = "stationary")$p.value, error = \(e) NA_real_)
  kpss_p <- tryCatch(kpss.test(x_ts, null = "Level")$p.value, error = \(e) NA_real_)

  nd_kpss <- tryCatch(ndiffs(x_ts, test = "kpss"), error = \(e) NA_integer_)
  nd_adf  <- tryCatch(ndiffs(x_ts, test = "adf"),  error = \(e) NA_integer_)
  ns_ocsb <- tryCatch(nsdiffs(x_ts, test = "ocsb"), error = \(e) NA_integer_)
  ns_seas <- tryCatch(nsdiffs(x_ts, test = "seas"), error = \(e) NA_integer_)

  tibble(
    dataset = label,
    metric = c(
      "Observations (n)",
      "Start",
      "End",
      "Suggested seasonal diffs D (OCSB)",
      "Suggested seasonal diffs D (SEAS)",
      "Suggested nonseasonal diffs d (KPSS)",
      "Suggested nonseasonal diffs d (ADF)",
      "ADF p-value (H0: unit root)",
      "KPSS p-value (H0: level stationary)"
    ),
    value = c(
      length(x_ts),
      paste(start(x_ts), collapse = ", "),
      paste(end(x_ts), collapse = ", "),
      ns_ocsb,
      ns_seas,
      nd_kpss,
      nd_adf,
      fmt_p_peak(adf_p),
      fmt_p_peak(kpss_p)
    )
  )
}

premodel_long <- bind_rows(
  premodel_checks(train_pp_bc_ts, "Pre-Pandemic (PP)"),
  premodel_checks(train_fh_bc_ts, "Full History (FH)")
)

premodel_wide <- premodel_long |>
  pivot_wider(names_from = dataset, values_from = value) |>
  rename(`Pre-modeling check` = metric)

# Renderable table object (don’t print here)
premodel_checks_kbl <- premodel_wide |>
  kable(
    format = if (is_latex_output()) "latex" else "html",
    booktabs = TRUE,
    align = c("l", "c", "c")
  ) |>
  add_header_above(c(" " = 1, "Training set" = 2)) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: tbl-premodel-checks
#| tbl-cap: "Pre-modeling checks on Box-Cox transformed training sets."

premodel_checks_kbl

if (ctrl$outputs) {
  save_rds_if(
    premodel_wide,
    here("output", "tables", "premodel_checks_tbl.rds"),
    ctrl
  )
}
```

Differencing diagnostics gave mixed recommendations, especially for the Full History series. We therefore treat differencing orders as candidate hyperparameters and select among them using out-of-sample forecast performance on the held-out season, alongside residual diagnostics.

# Model Building

```{r}
#| label: helper_eval-metrics

score_ts_model <- function(fit, dataset, tag , biasadj = TRUE) {
  s <- get_series(dataset)
  
  fc <- forecast(fit, h = length(s$test), biasadj = biasadj)
  
  acc <- accuracy(fc, s$test)
  
  # Enter default (S)ARIMA parameters in case using HW model
  p <- NA_real_
  d <- NA_real_
  q <- NA_real_
  P <- NA_real_
  D <- NA_real_
  Q <- NA_real_
  AICc <- NA_real_
  
  if (inherits(fit, "Arima")) {
    arma <- fit$arma
    p <- arma[1]
    q <- arma[2]
    P <- arma[3]
    Q <- arma[4]
    d <- arma[6]
    D <- arma[7]
    AICc <- fit$aicc
  }
  
  tibble(
    tag = tag, dataset = dataset,
    p = p, d = d, q = q,
    P = P, D = D, Q = Q,
    AICc = AICc,
    RMSE_train = unname(acc["Training set", "RMSE"]),
    MAE_train  = unname(acc["Training set", "MAE"]),
    RMSE_test  = unname(acc["Test set",   "RMSE"]),
    MAE_test   = unname(acc["Test set",   "MAE"])
  )
}
```

## Seasonal Naive Model
```{r}
#| label: model_seas-naive

# Helper to get correct train/test series and lambda
get_series <- function(dataset) {
  if (dataset == "pp") {
    list(train = train_pp_ts, test = test_pp_ts, lambda = lambda_pp)
  } else {
    list(train = train_fh_ts, test = test_fh_ts, lambda = lambda_fh)
  }
}

fit_snaive <- function(dataset) {
  s <- get_series(dataset)
  
  Arima(
    s$train,
    order = c(0, 0, 0),
    seasonal = c(0, 1, 0),
    lambda = s$lambda,
    include.constant = FALSE
  )
}

fit_pp_snaive <- fit_snaive("pp")
fit_fh_snaive <- fit_snaive("fh")
```

```{r}
#| label: metrics_snaive

metrics_pp_snaive <- score_ts_model(fit_pp_snaive, "pp", "pp_snaive")
metrics_fh_snaive <- score_ts_model(fit_fh_snaive, "fh", "fh_snaive")

metrics_pp_snaive
metrics_fh_snaive
```


## Holt-Winters Model
```{r}
#| label: model_holt-winters

fit_hw <- function(dataset) {
  s <- get_series(dataset)
  
  HoltWinters(x = s$train, seasonal = "additive")
}

fit_pp_hw <- fit_hw("pp")
fit_fh_hw <- fit_hw("fh")
```

```{r}
#| label: metrics_hw

metrics_pp_hw <- score_ts_model(fit_pp_hw, "pp", "pp_hw", biasadj = FALSE)
metrics_fh_hw <- score_ts_model(fit_fh_hw, "fh", "fh_hw", biasadj = FALSE)

metrics_pp_hw
metrics_fh_hw
```



## SARIMA
```{r}
#| label: model_search
#| eval: false

# Define the models to run
cfg <- tribble(
  ~dataset, ~d, ~D, ~tag,
  "pp",      0,  0, "pp_d0_D0",
  "pp",      0,  1, "pp_d0_D1",
  "fh",      0,  0, "fh_d0_D0",
  "fh",      0,  1, "fh_d0_D1",
  "fh",      1,  0, "fh_d1_D0",
  "fh",      1,  1, "fh_d1_D1",
  "pp",     NA, NA, "pp_auto",
  "fh",     NA, NA, "fh_auto"
)


# Fit SARIMA model for a given model
fit_model <- function(dataset, d, D, tag) {
  s <- get_series(dataset)

  if (is.na(d) || is.na(D)) {
    fit <- auto.arima(
      s$train,
      seasonal     = TRUE,
      lambda       = s$lambda,
      biasadj      = TRUE,
      stepwise     = FALSE,
      approximation = FALSE,
      max.p = 3, max.q = 3, max.P = 2, max.Q = 2
    )
  } else {
    fit <- auto.arima(
      s$train,
      d = d, D = D,
      seasonal     = TRUE,
      lambda       = s$lambda,
      biasadj      = TRUE,
      stepwise     = FALSE,
      approximation = FALSE,
      max.p = 3, max.q = 3, max.P = 2, max.Q = 2
    )
  }

  fit
}

# Prevents one model having issues from making the whole time spent running models wasted
safe_fit_model <- safely(fit_model, otherwise = NULL, quiet = TRUE)

# Fit all models from `cfg` in parallel and cache the models for later use
model_search_out <- cache_computation({
  
  out <- cfg |>
    mutate(
      res = future_pmap(
        .l = list(dataset, d, D, tag),
        .f = safe_fit_model,
        .options = furrr_options(seed = TRUE)
      )
    ) |>
    mutate(
      fit = map(res, "result"),
      err = map(res, "error")
    )
  
  # Keep successful model fits
  models_tbl <- out |>
    filter(map_lgl(err, is.null)) |>
    transmute(
      tag, dataset, d, D, fit
    )

    # Document failed models + error messages
  errors_tbl <- out |>
    filter(!map_lgl(err, is.null)) |>
    transmute(
      tag, dataset, d, D,
      error = map_chr(err, ~ conditionMessage(.x))
    )

  # What gets cached
  list(
    models_tbl = models_tbl,
    errors_tbl = errors_tbl
  )
},
cache_filename = "arima_model_search_models_cache.rds",
deps = list(
  cfg = cfg,
  lambda_pp = lambda_pp,
  lambda_fh = lambda_fh,
  pp_start = start(train_pp_ts), pp_end = end(train_pp_ts),
  fh_start = start(train_fh_ts), fh_end = end(train_fh_ts),
  pp_test_start = start(test_pp_ts), pp_test_end = end(test_pp_ts),
  fh_test_start = start(test_fh_ts), fh_test_end = end(test_fh_ts)
),
parallel = TRUE)

models_tbl <- model_search_out$models_tbl
errors_tbl <- model_search_out$errors_tbl

models_tbl
errors_tbl
```

```{r}
model_search_out <- readRDS(here("output", "arima_model_search_models_cache.rds"))$result
models_tbl <- model_search_out$models_tbl
errors_tbl <- model_search_out$errors_tbl
```

```{r}
#| label: build-sarima-metrics-table

# Compute metrics for each SARIMA candidate
sarima_metrics_tbl <- models_tbl |> 
  mutate(
    metrics = pmap(
      list(fit = fit, dataset = dataset, tag = tag),
      \(fit, dataset, tag) score_ts_model(fit = fit, dataset = dataset, tag = tag)
    )
  ) |> 
  transmute(metrics) |> 
  unnest(cols = metrics) |> 
  arrange(dataset, AICc)

# Formatted table object (don’t print here)
sarima_metrics_kbl <- sarima_metrics_tbl |>
  select(-tag) |>
  kable(
    format = if (is_latex_output()) "latex" else "html",
    digits   = 3,
    booktabs = TRUE
  ) |>
  add_header_above(c(
    " " = 1,
    "Model parameters" = 6,
    "In-Sample Accuracy" = 3,
    "Out-of-Sample Accuracy" = 2
  )) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: tbl-sarima-metrics
#| tbl-cap: "SARIMA candidate models and forecast accuracy by dataset."

sarima_metrics_kbl

if (ctrl$outputs) {
  save_rds_if(
    sarima_metrics_tbl,
    here("output", "tables", "sarima_metrics_tbl.rds"),
    ctrl
  )
}
```
```{r}
#| label: build-best-sarima

best_by_aicc <- sarima_metrics_tbl |>
  filter(!is.na(AICc)) |>
  group_by(dataset) |>
  slice_min(AICc, n = 1, with_ties = FALSE) |>
  ungroup() |>
  select(dataset, best_tag = tag, best_AICc = AICc)

# Pull the fits associated with the best tags
best_fits <- best_by_aicc |>
  left_join(models_tbl |> select(tag, fit), by = c("best_tag" = "tag"))

fit_pp_sarima <- best_fits |>
  filter(dataset == "pp") |>
  pull(fit)
fit_pp_sarima <- fit_pp_sarima[[1]]

fit_fh_sarima <- best_fits |>
  filter(dataset == "fh") |>
  pull(fit)
fit_fh_sarima <- fit_fh_sarima[[1]]

# Metrics (keep your human-friendly tags)
metrics_pp_sarima <- score_ts_model(fit_pp_sarima, "pp", "pp_sarima")
metrics_fh_sarima <- score_ts_model(fit_fh_sarima, "fh", "fh_sarima")
```

Rough SARIMA model breakdown
- Models look at last two weeks' ILI levels (p = 2)
  - $t-1, t-2$
- Models look at last two years of seasonal data for the same week (P = 2)
  - $t-52, t-104$
- Models look at noise from previous week
  - $t-1$

```{r}
#| label: build-model-comparison-table

all_metrics <- bind_rows(
  metrics_pp_snaive,
  metrics_pp_hw,
  metrics_pp_sarima,
  metrics_fh_snaive,
  metrics_fh_hw,
  metrics_fh_sarima
)

# Renderable table object (don’t print here)
all_metrics_kbl <- all_metrics |>
  kable(
    format = if (is_latex_output()) "latex" else "html",
    digits = 3,
    booktabs = TRUE
  ) |>
  add_header_above(c(
    " " = 2,
    "Model parameters" = 6,
    "In-Sample Accuracy" = 3,
    "Out-of-Sample Accuracy" = 2
  )) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: tbl-model-comparison
#| tbl-cap: "Model comparison across datasets using training and test accuracy."

all_metrics_kbl

if (ctrl$outputs) {
  save_rds_if(
    all_metrics,
    here("output", "tables", "all_model_metrics_tbl.rds"),
    ctrl
  )
}
```


# Model Prediction
## Cumulative Mean Average Error

Due to the possibility of the seasonally naive model getting lucky with a really good estimate right off the bat, I'll decide a priori a minimum number of weeks to include in the CMAE calculation ($h_{min}$) to evaluate before considering if the more sophisticated models actually perform better than the seasonal naive model. To be objective about this, I'm going to define $h_{min}$ before running any analysis, and base it off of typical staffing timelines and supply ordering timelines that might be present in a medical office. Most skilled medical staff in the US work in situations where the schedule must be posted 4-6 weeks in advance [@wangEvaluatingImpactFlexibility2009]. I couldn't find anything concrete on timelines of requesting travel nurses and them arriving, however travel nurse contracts usually vary from about 2-26 weeks in length [@rnHowLongAre2025], and nurses planning on working as travel nurses are given the advice that it usually takes 1-5 weeks between beginning to apply for traveling nursing jobs and actually starting at them [@schmidtHowLongDoes2018]. For the time it takes between ordering and receiving supplies, typically this is very low, however in times of increased demand (e.g., the 2009 H1N1 flu season), delivery times for PPE were increased in the range of 2 to 10 weeks [@patelPersonalProtectiveEquipment2017]. Taking all of these together, I will set the minimum horizon for evaluating CMAE to $h_{min} = 4$ weeks. That is, I will only consider the more complex models to have a meaningful advantage over the seasonal naive model if their CMAE is lower from at least week 4 onwards.


```{r}
#| label: helper_cmae

# Helper to compute CMAE(h) for a model
compute_cmae <- function(fit, dataset, model_id, model, h_max = 52, biasadj = TRUE) {
  s <- get_series(dataset)
  H <- min(h_max, length(s$test))
  y_true <- as.numeric(s$test)[seq_len(H)]

  fc <- if (inherits(fit, "forecast")) fit else forecast(fit, h = H, biasadj = biasadj)
  y_pred <- as.numeric(fc$mean)[seq_len(H)]

  abs_err <- abs(y_true - y_pred)

  tibble(
    dataset = dataset,
    model_id = model_id,
    model = model,
    h = seq_len(H),
    abs_err = abs_err,
    CMAE = cumsum(abs_err) / h
  )
}

# Helper to compute h* for a model vs seasonal naive baseline
eff_horiz <- function(cmae_model, cmae_snaive, h_min = 4L) {
  joined <- inner_join(
    cmae_model  |> select(dataset, h, CMAE_model  = CMAE),
    cmae_snaive |> select(dataset, h, CMAE_snaive = CMAE),
    by = c("dataset", "h")
  )
  
  ok <- joined |>
    filter(
      h >= h_min,
      CMAE_model <= CMAE_snaive
    )
  
  if (nrow(ok) == 0L) {
    return(0L)
  }
  
  max(ok$h)
}

# Helper to create plots
make_fc_df <- function(fit, dataset, model_id, model, h_max = 52, level = 95, biasadj = TRUE) {
  s <- get_series(dataset)
  H <- min(h_max, length(s$test))

  fc <- forecast(fit, h = H, level = level, biasadj = biasadj)

  tibble(
    dataset     = dataset,
    model_id    = model_id,
    model       = model,
    season_week = seq_len(H),
    y_true      = as.numeric(s$test)[seq_len(H)],
    mean        = as.numeric(fc$mean),
    lo          = as.numeric(fc$lower[, 1]),
    hi          = as.numeric(fc$upper[, 1])
  )
}
```

```{r}
#| label: build-forecast-cmae-config

h_max <- 52
h_min <- 4L

model_cfg <- tribble(
    ~model_id, ~model,           ~biasadj,
    "snaive",  "Seasonal naive", TRUE,
    "hw",      "Holt-Winters",   FALSE,
    "sarima",  "SARIMA",         TRUE
  )

fit_tbl <- bind_rows(
    tibble(dataset = "pp", model_id = "snaive", fit = list(fit_pp_snaive)),
    tibble(dataset = "pp", model_id = "hw",     fit = list(fit_pp_hw)),
    tibble(dataset = "pp", model_id = "sarima", fit = list(fit_pp_sarima)),
    tibble(dataset = "fh", model_id = "snaive", fit = list(fit_fh_snaive)),
    tibble(dataset = "fh", model_id = "hw",     fit = list(fit_fh_hw)),
    tibble(dataset = "fh", model_id = "sarima", fit = list(fit_fh_sarima))
  ) |>
  left_join(model_cfg, by = "model_id")
```

```{r}
#| label: build-forecast-cmae-data

cmae_all <- pmap_dfr(fit_tbl, \(dataset, model_id, fit, model, biasadj) {
  compute_cmae(
    fit = fit,
    dataset = dataset,
    model_id = model_id,
    model = model,
    h_max = h_max,
    biasadj = biasadj
  )
})

fc_all <- pmap_dfr(fit_tbl, \(dataset, model_id, fit, model, biasadj) {
  make_fc_df(
    fit = fit,
    dataset = dataset,
    model_id = model_id,
    model = model,
    h_max = h_max,
    biasadj = biasadj
  )
})

obs_all <- fc_all |>
  distinct(dataset, season_week, y_true)
```

```{r}
#| label: build-effective-horizon

cmae_baseline <- cmae_all |>
  filter(model_id == "snaive") |>
  select(dataset, h, CMAE_snaive = CMAE)

cmae_models <- cmae_all |>
  filter(model_id != "snaive") |>
  select(dataset, model_id, model, h, CMAE_model = CMAE) |>
  left_join(cmae_baseline, by = c("dataset", "h"))

eff_horiz_tbl <- cmae_models |>
  filter(h >= h_min, CMAE_model <= CMAE_snaive) |>
  group_by(dataset, model_id, model) |>
  summarise(h_star = max(h), h_min = h_min, .groups = "drop")

# Mark points for plotting
eff_marks_plot <- eff_horiz_tbl |>
  left_join(
    cmae_all,
    by = c("dataset", "model_id", "model", "h_star" = "h")
  ) |>
  rename(CMAE_star = CMAE)

eff_horizon_kbl <- eff_horiz_tbl |>
  mutate(dataset = recode(dataset, pp = "Pre-pandemic (PP)", fh = "Full history (FH)")) |>
  select(
    Dataset = dataset, Model = model,
    `h_min` = h_min, `h*` = h_star
  ) |>
  kable(booktabs = TRUE, align = c("l", "l", "c", "c")) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: tbl-effective-horizon
#| tbl-cap: "Effective forecast horizon h* where CMAE(h) is no worse than the seasonal naive baseline."

eff_horizon_kbl

if (ctrl$outputs)
  save_rds_if(eff_horiz_tbl, here("output", "tables", "eff_horiz_tbl.rds"), ctrl
  )
```


```{r}
#| label: build-forecast-cmae-scales

model_levels <- c("Observed", model_cfg$model)

model_cols <- c(
  "Observed"       = "black",
  "Seasonal naive" = nav_cols[4],
  "Holt-Winters"   = nav_cols[2],
  "SARIMA"         = nav_cols[1]
)

model_lty <- c(
  "Observed"       = "solid",
  "Seasonal naive" = "solid",
  "Holt-Winters"   = "dotted",
  "SARIMA"         = "dashed"
)

model_lw <- c(
  "Observed"       = 1.2,
  "Seasonal naive" = 0.9,
  "Holt-Winters"   = 0.9,
  "SARIMA"         = 0.9
)

```

```{r}
#| label: build-forecast-cmae-plots

plot_test_fc <- function(dataset_label, dataset_code) {
  obs <- obs_all |> filter(dataset == dataset_code)
  fc  <- fc_all  |> filter(dataset == dataset_code)

  ggplot() +
    geom_line(
      data = obs,
      aes(season_week, y_true,
                   colour = "Observed", linetype = "Observed", linewidth = "Observed")
    ) +
    geom_ribbon(
      data = fc |> filter(model_id != "snaive"),
      aes(season_week, ymin = lo, ymax = hi, fill = model),
      alpha = 0.15
    ) +
    geom_line(
      data = fc,
      aes(season_week, mean,
                   colour = model, linetype = model, linewidth = model)
    ) +
    scale_colour_manual(values = model_cols, breaks = model_levels, drop = FALSE) +
    scale_fill_manual(values = model_cols, breaks = model_levels[-1], drop = FALSE) +
    scale_linetype_manual(values = model_lty, breaks = model_levels, drop = FALSE) +
    scale_linewidth_manual(values = model_lw, breaks = model_levels, drop = FALSE) +
    labs(
      title = dataset_label,
      x = "Week into flu season",
      y = "Weighted ILI (%)",
      colour = "Model", fill = "Model", linetype = "Model", linewidth = "Model"
    ) +
    theme_minimal()
}

plot_cmae <- function(dataset_label, dataset_code) {
  d <- cmae_all |> filter(dataset == dataset_code)
  marks <- eff_marks_plot |> filter(dataset == dataset_code)

  ggplot(
    d,
    aes(h, CMAE, colour = model, linetype = model, linewidth = model)
  ) +
    geom_line() +
    geom_vline(xintercept = h_min, linetype = "dotted", colour = "grey40") +
    geom_point(
      data = marks,
      aes(x = h_star, y = CMAE_star, colour = model),
      size = 2
    ) +
    scale_colour_manual(values = model_cols, breaks = model_levels, drop = FALSE) +
    scale_linetype_manual(values = model_lty, breaks = model_levels, drop = FALSE) +
    scale_linewidth_manual(values = model_lw, breaks = model_levels, drop = FALSE) +
    guides(
      colour = "none",
      linetype = "none",
      linewidth = "none",
      fill = "none"
    ) +
    labs(
      title = paste0(dataset_label, ": CMAE by horizon"),
      x = "Forecast horizon h (weeks ahead)",
      y = "CMAE(h), Weighted ILI (%)"
    ) +
    theme_minimal()
}

gg_pp_test <- plot_test_fc("Pre-pandemic test period", "pp")
gg_fh_test <- plot_test_fc("Full-history test period", "fh")

gg_pp_cmae <- plot_cmae("Pre-pandemic", "pp")
gg_fh_cmae <- plot_cmae("Full-history", "fh")

```

```{r}
#| label: build-forecast-cmae-grid
#| include: false

forecast_cmae_grid <-
  (gg_pp_test | gg_pp_cmae) /
  (gg_fh_test | gg_fh_cmae) +
  plot_annotation(title = "Forecast performance by model and dataset",
                             subtitle = "Left: forecasts over the test season. Right: CMAE as horizon increases.") +
  plot_layout(guides = "collect") &
  theme(legend.position = "right")
```

```{r}
#| label: fig-forecast-cmae-grid
#| fig-cap: "Forecast performance by model and dataset."
#| fig-width: 7
#| fig-asp: 0.8

forecast_cmae_grid

if (ctrl$outputs) {
  save_plot(forecast_cmae_grid, "forecast_cmae_grid")
}
```

# Residual Diagnostics
```{r}
#| label: build-residual-diagnostics

make_resid_panel <- function(fit, panel_title, lag_max = 104, bins = 50) {
  r_ts <- residuals(fit)
  r <- as.numeric(r_ts)

  df_ts <- tibble(time = as.numeric(time(r_ts)), resid = r)

  p_time <- ggplot(df_ts, aes(time, resid)) +
    geom_line(linewidth = 0.35) +
    geom_hline(yintercept = 0, linewidth = 0.3) +
    labs(title = panel_title, x = NULL, y = "Residual") +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(face = "bold", size = 14, hjust = 0, margin = margin(b = 6)),
      plot.title.position = "plot"
    )

  m <- mean(r, na.rm = TRUE)
  s <- sd(r, na.rm = TRUE)

  p_hist <- ggplot(tibble(resid = r), aes(resid)) +
    geom_histogram(aes(y = after_stat(density)), bins = bins) +
    stat_function(fun = dnorm, args = list(mean = m, sd = s), linewidth = 0.35) +
    labs(x = "Residual", y = "Density") +
    theme_minimal(base_size = 10)

  p_acf <- ggAcf(r, lag.max = lag_max) +
    labs(title = "Residual ACF", x = "Lag (weeks)", y = "ACF") +
    theme_minimal(base_size = 10) +
    theme(plot.title = element_text(face = "bold", size = 11))

  (p_time + p_hist + p_acf) +
    plot_layout(design = "AA\nBC", heights = c(1.1, 1))
}

diag_pp <- make_resid_panel(fit_pp_sarima, "Pre-Pandemic (PP) SARIMA")
diag_fh <- make_resid_panel(fit_fh_sarima, "Full History (FH) SARIMA")

sarima_resid_diag_plot <-
  (diag_pp / diag_fh) +
  plot_annotation(
    title = "Residual diagnostics for SARIMA models",
    subtitle = "Time plot, residual distribution, and residual ACF"
  )

lb_pp <- Box.test(residuals(fit_pp_sarima), lag = 52, fitdf = 5, type = "Ljung-Box")
lb_fh <- Box.test(residuals(fit_fh_sarima), lag = 52, fitdf = 5, type = "Ljung-Box")
```

```{r}
#| label: fig-residual-diagnostics
#| fig-cap: "Residual diagnostics for SARIMA models (top: pre-pandemic; bottom: full history)."
#| fig-asp: 1

sarima_resid_diag_plot

if (ctrl$outputs) {
  save_plot(sarima_resid_diag_plot, "sarima_residual_diagnostics")
}
```

```{r}
#| label: tbl-ljung-box
#| tbl-cap: "Ljung–Box test results on SARIMA residuals (lag 52)."

lb_tbl <- tibble(
  dataset   = c("Pre-Pandemic (PP)", "Full History (FH)"),
  statistic = c(unname(lb_pp$statistic), unname(lb_fh$statistic)),
  df        = c(unname(lb_pp$parameter), unname(lb_fh$parameter)),
  p_value   = c(lb_pp$p.value, lb_fh$p.value)
)

lb_tbl |>
  kable(
    format = if (is_latex_output()) "latex" else "html",
    digits = 4,
    booktabs = TRUE
  ) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```