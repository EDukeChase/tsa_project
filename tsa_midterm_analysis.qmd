---
title: "Timerseries Analysis Midterm Analysis"
author: "E. Duke Chase"
date: today
---

# Load Packages
```{r}
#| label: setup
#| include: false

# General settings and ancillary packages
source("R/setup.R")
source("R/save_plot.R")
source("R/cache_computation.R")

# Project libraries
library(forecast)
library(tseries)

# Change default target directory and file formats for `save_plot()` function
save_plot_orig <- save_plot

make_save_plot <- function(fun, default_target = ".", default_format = "png") {
  force(fun)
  force(default_target)
  force(default_format)

  function(plot, name, format = default_format, target = default_target, ...) {
    fun(plot, name = name, format = format, target = target, ...)
  }
}

# Project defaults: output folder + png+svg
save_plot <- make_save_plot(
  fun = save_plot_orig,
  default_target = "output/figures",
  default_format = c("png", "svg")
)

# Default color palette
nav_cols <- RColorBrewer::brewer.pal(8, "Set2")
```

# Data Cleaning

```{r}
#| label: load_process

# Load data
ili_raw <- read.csv(here("data", "ILINet.csv"),
                    header = TRUE,
                    skip = 1)

# Clean data
ili_clean <- ili_raw |>
  # Remove rows with missing data or no unique data
  select(
    -REGION.TYPE,         # All are "National"
    -REGION,              # All are "X"
    -X.UNWEIGHTED.ILI,    # Raw, not weighted by state size
    -contains("AGE"),     # Two of the demographic breakdowns are empty
    -ILITOTAL,            # Raw values 
    -NUM..OF.PROVIDERS,   # Raw values
    -TOTAL.PATIENTS       # Raw values
  ) |>
  
  # Standardize variable naming
  rename(year = YEAR, week = WEEK, ili_w = X..WEIGHTED.ILI) |>  
  
  # Modify to work two-year spanning flu-seasons (start week is 40)
  mutate(
    flu_season = if_else(week >= 40, year, year - 1),
    season_week = if_else(week >= 40, week - 39, week + 13),
    season_label = paste0(flu_season, "-", flu_season + 1
    )
  ) |> 
  # Remove data from the incomplete 2025-2026 flu season
  filter(flu_season < 2025)
```

One issue I'll have to deal with right off the bat is that some years have 53 weeks and some have 52. For simplicity of analysis I'm going to start off by just dropping the 53rd week from years that have it, but I will go back and do a sensitivity analysis comparing a few methods (imputing 53rd weeks for other years, averaging the 53rd week across the 52nd and 1st of next year, etc.).

```{r}
#| label: week-53-problem

# Create working dataset for seasonality check
ili_seasonal <- ili_clean |>
  # Drop 53rd week to standardize length (naive approach, revisit later)
  filter(week <= 52) |> 
  arrange(year, week)

# Create ts object
ili_ts_raw <- ts(
  ili_seasonal$ili_w,
  start = c(ili_seasonal$year[1], ili_seasonal$week[1]),
  frequency = 52
)

raw_data_plot <- plot(ili_ts_raw, 
     main = "Weighted ILI Activity", 
     ylab = "Weighted ILI %")

raw_data_plot
```

Looking at this plot, it is apparent that until about 2003, data were only collected during flu season and not over the summer months, so those flu seasons will be excluded from the analysis.

```{r}
#| label: find-zero-weeks

zero_check <- ili_seasonal |> 
  group_by(flu_season) |> 
  summarize(
    weeks_with_zero = sum(ili_w == 0, na.rm = TRUE),
    min_ili = min(ili_w, na.rm = TRUE),
    total_weeks = n()
  )

print(zero_check, n = 8)
```

Based on this output, 2002 - 2003 is the first flu season with no missing weeks, so I will discard all data before that.
Further research showed that there was sporadic collection for the summer of 2003 [@CDCExpandMonitoring], and only data on children for the summers of 2004 and 2005 [@CDCSum03], so the 2005-06 flu season will be the first included in the dataset.

```{r}
#| label: drop-pre-2005-season

# Drop data before 2005 flu season
ili_df <- ili_seasonal |> 
  filter(flu_season >= 2005) |> 
  arrange(year, week)

# Overwrite previous ts object
ili_ts <- ts(
  ili_df$ili_w,
  start = c(2005, 40),
  frequency = 52
)

clean_data_plot <- plot(ili_ts, 
     main = "Weighted ILI Activity (Cleaned)", 
     ylab = "Weighted ILI %")

clean_data_plot
```

# Exploratory Analysis
```{r}
#| label: plot_seasons
#| fig-width: 12
#| fig-height: 8

highlight_years <- c("2009-2010", "2011-2012", "2017-2018", "2019-2020", 
                     "2020-2021", "2022-2023", "2024-2025")
                     
mini_colors <- brewer.pal(7, "Dark2")
mini_tags <- paste0("(", letters[1:7],")")
                     
p_main <- ggplot(ili_df, aes(x = season_week, y = ili_w, group = flu_season)) +
  geom_line(color = "grey60", alpha = 0.5, size = 0.5) +
  labs(
    x = "Weeks into Flu Season",
    y = "Weighted ILI%"
  )

create_mini_plot <- function(i) {
  
  target_year <- highlight_years[i]
  target_color <- mini_colors[i]
  target_tag <- mini_tags[i]
  
  mini_df <- ili_df |> 
    mutate(
      is_focus = if_else(season_label == target_year, "Focus", "Background")
    ) |> 
    arrange(is_focus)
  
  ggplot(mini_df, aes(x = season_week, y = ili_w, group = flu_season)) +
    geom_line(aes(color = is_focus, size = is_focus, alpha = is_focus)) +
    
    scale_color_manual(values = c("Background" = "grey90", "Focus" = target_color)) +
    scale_size_manual(values = c("Background" = 0.3, "Focus" = 1.0)) +
    scale_alpha_manual(values = c("Background" = 0.7, "Focus" = 1.0)) +
    
    theme_void() + 
    theme(
      axis.title.x = element_text(size = 9, face = "bold", margin = margin(t = 2, b = 10)),
      panel.border = element_rect(color = "grey90", fill = NA),
      plot.tag = element_text(size = 10, face = "bold"),
      plot.tag.position = c(0.95, 0.95)
    ) +
    labs(
      x = target_year,
      tag = target_tag
    ) + 
    guides(color = "none", size = "none", alpha = "none")
}

mini_plots <- lapply(1:7, create_mini_plot)

p1 <- mini_plots[[1]]
p2 <- mini_plots[[2]]
p3 <- mini_plots[[3]]
p4 <- mini_plots[[4]]
p5 <- mini_plots[[5]]
p6 <- mini_plots[[6]]
p7 <- mini_plots[[7]]

layout_design <- "
AAAB
AAAC
AAAD
EFGH
"

flu_overview <- p_main + p1 + p2 + p3 + p4 + p5 + p6 + p7 +
  plot_layout(design = layout_design) +
  plot_annotation(
    title = "Weighted ILI %: Historical Overview & Key Anomalies",
    subtitle = "Main plot (Left) shows the full historical range.\nSubplots (Right/Bottom) isolate notable seasons."
    )

flu_overview
```




In the main figure we can see that for most flu seasons there's a peak between 13-18 weeks into the season, but there is a reasonable amount of variation in this, with one peak happening as early as 3 weeks into the flu season, and another as late as 25 weeks into the season. This is definitely more variation than would be seen just due to deleting week 53 or the slight variation in how weeks line up year to year. Interestingly, despite the wide distribution of peaks during flu season, the trough of flu season almost universally occurs around week 30.

Around the main plot I've highlighted a few flu seasons that had interesting characteristics. Subplots (a) and (c) were both heavy flu years, with (a) reflecting the H1N1 (swine flu) epidemic; swine flu had a very early start in the year compared to typical flu variants, and is also the only time I've experienced hallucinations due to fever. Subplot (b) was noteworthy due to being an incredibly mild flu season. Subplot (g) was interesting because it had two distinct peaks during the season. Lastly, subplots (d), (e) and (f) are especially indicative of the effects COVID-19 had on flu occurrence. Subplot (d) shows the flu season which included the initial spread of COVID-19, which may explain why it has three peaks instead of one like the majority of seasons have, especially since early COVID-19 strains had significant symptom overlap with influenza, and this data is related to doctor visits, not tested strains. Subplot (e) occurred during the height of non-pharmaceutical interventions intended to slow the spread (masking, social distancing, etc.)[@covid_npi], showing that there was almost no detectable flu season as a result. Subplot (f) occurred soon after the vast majority of state-level interventions had been lifted[@covid_npi], and peaked earlier in the year than typical, possibly lending credence to the idea of immune debt [@immune_debt] causing people to be more vulnerable to infection after a period of low exposure.

At this point of the exploratory analysis, it's worth splitting into the two datasets I'll be using for the analysis., since the characteristics change depending on the time period in question due to just how big of a shock event COVID-19 was to ILI% numbers. 
```{r}
#| label: split_data-by-covid

# Create updated dataframes and timeseries
# Full History [2005 - 2025]
ili_fh <- ili_df                       
ili_fh_ts <- ili_ts

# Pre-Pandemic [2005 - 2019)
ili_pp <- ili_df |>                    
  filter(flu_season < 2019)
ili_pp_ts <- ts(
  ili_pp$ili_w,
  start = c(2005, 40),
  frequency = 52
)

# Pandemic Era [2019 - 2025]
ili_pe <- ili_df |>                    
  filter(flu_season >= 2019)

ili_pe_ts <- ts(
  ili_pe$ili_w,
  start = c(2019, 40),
  frequency = 52
)
```

The full history dataset covers flu seasons 2005-06 through 2024-25 ($\text{N}_{CI} = $`r nrow(ili_fh)`, $\text{S}_{CI} = $`r n_distinct(ili_fh$season_label)`), the pre-pandemic dataset covers flu seasons 2005-06 through 2018-19 ($\text{N}_{CE} = $ `r nrow(ili_pp)`, $\text{S}_{CE} = $`r n_distinct(ili_pp$season_label)`), and the pandemic era dataset covers flu seasons 2019-20 through 2024-25 ($\text{N}_{PE} = $`r nrow(ili_pe)`, $\text{S}_{PE} = $`r n_distinct(ili_pe$season_label)`). The pandemic era dataset is only used to compare the pre- and post-pandemic trends, but will not be analyzed on its own.


```{r}
#| label: data-metrics-and-comparison

season_peaks_tbl <- function(df) {
  df |>
    group_by(flu_season, season_label) |>
    summarise(
      peak_ili   = max(ili_w, na.rm = TRUE),
      peak_week  = season_week[which.max(ili_w)],
      trough_ili = min(ili_w, na.rm = TRUE),
      amplitude  = peak_ili - trough_ili,
      .groups = "drop"
    )
}

peak_summary_tbl <- function(season_peaks) {
  season_peaks |>
    summarise(
      seasons           = n(),
      peak_week_median  = median(peak_week, na.rm = TRUE),
      peak_week_iqr     = IQR(peak_week, na.rm = TRUE),
      peak_week_min     = min(peak_week, na.rm = TRUE),
      peak_week_max     = max(peak_week, na.rm = TRUE),
      peak_ili_median   = median(peak_ili, na.rm = TRUE),
      peak_ili_iqr      = IQR(peak_ili, na.rm = TRUE),
      amplitude_median  = median(amplitude, na.rm = TRUE),
      amplitude_iqr     = IQR(amplitude, na.rm = TRUE),
      .groups = "drop"
    ) |>
    mutate(
      peak_week_minmax = paste0(peak_week_min, "-", peak_week_max)
    ) |>
    select(
      seasons,
      peak_week_median, peak_week_iqr, peak_week_minmax,
      peak_ili_median, peak_ili_iqr,
      amplitude_median, amplitude_iqr
    )
}

peak_long_tbl <- function(peak_summary) {
  peak_summary |>
    (\(x) enframe(as.list(x), name = "Metric", value = "Value"))() |>
    mutate(
      Metric = recode(
        Metric,
        seasons          = "Number of seasons",
        peak_week_median = "Peak week (median)",
        peak_week_iqr    = "Peak week (IQR)",
        peak_week_minmax = "Peak week (min-max)",
        peak_ili_median  = "Peak ILI% (median)",
        peak_ili_iqr     = "Peak ILI% (IQR)",
        amplitude_median = "Season amplitude (median)",
        amplitude_iqr    = "Season amplitude (IQR)",
        .default = Metric
      ),
      Value = map_chr(Value, \(v) {
        if (is.numeric(v)) sprintf("%.2f", v) else as.character(v)
      })
    )
}

fmt_p <- function(p) {
  if (is.na(p)) return("")
  if (p < 0.001) return("< 0.001")
  sprintf("%.3f", p)
}

# ---- build season-level peaks for each dataset ----
peaks_pp <- season_peaks_tbl(ili_pp)
peaks_pe <- season_peaks_tbl(ili_pe)
peaks_fh <- season_peaks_tbl(ili_fh)

# Welch Test (PP vs PE) on season-level metrics
welch_pvals <- tibble( 
  Metric = c("Peak ILI% (median)", "Season amplitude (median)", "Peak week (median)"), 
  `Welch p (PP vs PE)` = c( 
    fmt_p(t.test(peaks_pp$peak_ili, peaks_pe$peak_ili, alternative = "two.sided")$p.value),
    fmt_p(t.test(peaks_pp$amplitude, peaks_pe$amplitude, alternative = "two.sided")$p.value),
    fmt_p(t.test(peaks_pp$peak_week, peaks_pe$peak_week, alternative = "two.sided")$p.value) 
    ) 
  )

# ---- summaries into long form ----
ce_long <- peak_long_tbl(peak_summary_tbl(peaks_pp)) |> rename(`Pre-Pandemic (2005-2018)` = Value)
co_long <- peak_long_tbl(peak_summary_tbl(peaks_pe)) |> rename(`Pandemic Era (2019-2025)`       = Value)
ci_long <- peak_long_tbl(peak_summary_tbl(peaks_fh)) |> rename(`Full History (2005-2025)`= Value)

# ---- combine ----
peak_compare_tbl <- ce_long |>
  left_join(co_long, by = "Metric") |>
  left_join(welch_pvals, by = "Metric") |> # Put P-value next to the Pandemic column
  left_join(ci_long, by = "Metric") |>      # Put Full History last as reference
  mutate(
    # Optional: Replace NA p-values with empty strings for a cleaner look
    `Welch p (PP vs PE)` = replace_na(`Welch p (PP vs PE)`, "")
  ) |>
  rename(
    `Season-level summary` = Metric,
    `P-value` = `Welch p (PP vs PE)` # Shorten name since header will add context
  )

# ---- render ----
peak_compare_tbl |>
  kable(
    format = "latex",
    booktabs = TRUE,
    align = c("l", "c", "c", "c", "c"), # Center align data columns
    caption = "Season-level peak timing and intensity summaries."
  ) |>
  # CHANGE 2: Add a spanning header to explicitly link the test to the groups
  add_header_above(c(
    " " = 1, 
    "Cohort Comparison (Welch t-test)" = 3, 
    "Reference" = 1
  )) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r}
#| label: variability-plot

make_season_profile <- function(df) {
  df |>
    group_by(season_week) |>
    summarise(
      med = median(ili_w, na.rm = TRUE),
      q25 = quantile(ili_w, 0.25, na.rm = TRUE),
      q75 = quantile(ili_w, 0.75, na.rm = TRUE),
      .groups = "drop"
    )
}

make_profile_plot <- function(df, title) {
  prof <- make_season_profile(df)

  ggplot(prof, aes(x = season_week)) +
    geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2) +
    geom_line(aes(y = med), linewidth = 0.8) +
    labs(
      title = title,
      subtitle = "Median ILI% with interquartile range across seasons",
      x = "Weeks into flu season",
      y = "Weighted ILI%"
    ) +
    theme_minimal()
}

p_fh <- make_profile_plot(ili_fh, "Full History (2005-2025)")
p_pp <- make_profile_plot(ili_pp, "Pre-Pandemic (2005-2018)")
p_pe <- make_profile_plot(ili_pe, "Pandemic Era (2019-2025)")

# Use shared scales
y_max <- max(
  make_season_profile(ili_fh)$q75,
  make_season_profile(ili_pp)$q75,
  make_season_profile(ili_pe)$q75,
  na.rm = TRUE
)

p_fh <- p_fh + coord_cartesian(ylim = c(0, y_max))
p_pp <- p_pp + coord_cartesian(ylim = c(0, y_max))
p_pe <- p_pe + coord_cartesian(ylim = c(0, y_max))

# --- THE FIX ---
# We create a 4-column grid.
# Row 1: Empty (#), Plot A (2 cols), Empty (#)
# Row 2: Plot B (2 cols), Plot C (2 cols)
layout <- "
#AA#
BBCC
"

peak_var_plot <- (p_fh + p_pp + p_pe) +
  plot_layout(design = layout) +
  plot_annotation(
    title = "Typical flu-season pattern and variability by week",
    subtitle = "Top: Full history (centered). Bottom: Pre- vs Post-COVID-19 comparison."
  )

peak_var_plot
```

# Analysis Prep
## Train-Test Split
```{r}
#| label: train-test-split

# Testing data with COVID-19 excluded
ili_pp_test <- ili_pp |> 
  filter(season_label == "2018-2019")
test_pp_ts <- ts(
  ili_pp_test$ili_w,
  start = c(ili_pp_test$year[1], ili_pp_test$week[1]),
  frequency = 52
)

# Training data with COVID-19 excluded
ili_pp_train <- ili_pp |> 
  filter(season_label != "2018-2019")
train_pp_ts <- ts(
  ili_pp_train$ili_w,
  start = c(ili_pp_train$year[1], ili_pp_train$week[1]),
  frequency = 52
)

# Testing data with COVID-19 included
ili_fh_test <- ili_fh |> 
  filter(season_label == "2024-2025")
test_fh_ts <- ts(
  ili_fh_test$ili_w,
  start = c(ili_fh_test$year[1], ili_fh_test$week[1]),
  frequency = 52
)

# Training data with COVID-19 included
ili_fh_train <- ili_fh |> 
  filter(season_label != "2024-2025")
train_fh_ts <- ts(
  ili_fh_train$ili_w,
  start = c(ili_fh_train$year[1], ili_fh_train$week[1]),
  frequency = 52
)
```

## Variance Stabilization
```{r}
#| label: boxcox_transform_and_plot
#| fig-width: 12
#| fig-height: 6

# Estimate lambda on training data
lambda_pp <- BoxCox.lambda(as.numeric(train_pp_ts), method = "guerrero")
lambda_fh <- BoxCox.lambda(as.numeric(train_fh_ts), method = "guerrero")

# Store the lambdas in a way that they are easily retrievable for back-transformation
bc_params <- tibble(
  dataset = c("pp", "fh"),
  lambda_hat = c(lambda_pp, lambda_fh),
  method = "guerrero"
)
saveRDS(bc_params, here("data", "boxcox_params_train_only.rds"))

# Apply BoxCox transformation to training data
train_pp_bc_ts <- BoxCox(train_pp_ts, lambda_pp)
train_fh_bc_ts <- BoxCox(train_fh_ts, lambda_fh)

# Helper functions for plots
ts_to_df <- function(x) tibble(time = as.numeric(time(x)), value = as.numeric(x))

make_ts_plot <- function(x, title, ylab) {
  ggplot(ts_to_df(x), aes(time, value)) +
    geom_line(linewidth = 0.35) +
    labs(title = title, x = NULL, y = ylab) +
    theme_minimal()
}

# Create the plots
p_pp_raw <- make_ts_plot(train_pp_ts, "Pre-Pandemic: raw", "ILI%")
p_fh_raw <- make_ts_plot(train_fh_ts, "Full History: raw", "ILI%")

p_pp_bc <- make_ts_plot(
  train_pp_bc_ts,
  bquote("Pre-Pandemic: BoxCox (" * hat(lambda) == .(sprintf("%.3f", lambda_pp)) * ")"),
  "BoxCox(ILI%)"
)
p_fh_bc <- make_ts_plot(
  train_fh_bc_ts,
  bquote("Full History: BoxCox (" * hat(lambda) == .(sprintf("%.3f", lambda_fh)) * ")"),
  "BoxCox(ILI%)"
)

data_transformation_plot <- (p_pp_raw | p_fh_raw) /
(p_pp_bc  | p_fh_bc) +
  plot_annotation(
    title = "Variance stabilization on training sets (Box-Cox)",
    subtitle = bquote(hat(lambda) * " estimated with Guerrero method")
  )

data_transformation_plot
```

As can be seen, the transformed training datasets visible in the bottom row exhibit much less variability in their peaks over the years. There is a loss of stability in terms of the lowest values, as the summer months were very constant on the original scale, but this is worth the reduction in variability of the peaks.

```{r}
#| label: stl_decomposition
#| fig-width: 12
#| fig-height: 6

stl_pp <- stl(train_pp_bc_ts, s.window = "periodic", robust = TRUE)
stl_fh <- stl(train_fh_bc_ts, s.window = "periodic", robust = TRUE)

p_stl_pp <- autoplot(stl_pp) + ggtitle("STL decomposition (PP train, Box-Cox)")
p_stl_fh <- autoplot(stl_fh) + ggtitle("STL decomposition (FH train, Box-Cox)")

decomp_plot <- (p_stl_pp | p_stl_fh) +
  plot_annotation(title = "STL decomposition on transformed training series")

decomp_plot
```

The decompositions for the two datasets both look okay, although obviously not perfect because we'd want a flat trend line and for the remainders to look more like white noise. As it is, the trend line is all over the place, but the white noise looks decent to me when you consider that we are dealing with a variable, biological process. The biggest outliers are around 2009-2010 (H1N1) and 2020-2024 (COVID-19); other than that there are small variations which clearly aren't white noise, but are pretty minimal and reflect slight shifts in when exactly the flu season starts and peaks in a given year.


```{r}
#| label: plot_acf-pacf
#| fig-asp: 1.2

acf_theme <- theme_bw() +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 11),
    axis.text  = element_text(size = 9)
  )

seasonal_highlight <- list(
  annotate("rect",
    xmin = 50, xmax = 54,
    ymin = -Inf, ymax = Inf,
    fill = brewer.pal(3, "Dark2")[1],
    alpha = 0.2
  ),
  geom_vline(
    xintercept = 52,
    color = brewer.pal(3, "Dark2")[1],
    linetype = "dashed",
    alpha = 0.6
  )
)

make_acf_pacf_set <- function(x_ts, col_label) {
  x <- as.numeric(x_ts)

  acf_st  <- ggAcf(x, lag.max = 26) +
    labs(title = paste0(col_label, ": Short-term ACF"), x = "Lag (weeks)") + acf_theme

  pacf_st <- ggPacf(x, lag.max = 26) +
    labs(title = paste0(col_label, ": Short-term PACF"), x = "Lag (weeks)") + acf_theme

  acf_lt  <- ggAcf(x, lag.max = 104) + seasonal_highlight +
    labs(title = paste0(col_label, ": Long-term ACF (2 years)"), x = "Lag (weeks)") + acf_theme

  pacf_lt <- ggPacf(x, lag.max = 104) + seasonal_highlight +
    labs(title = paste0(col_label, ": Long-term PACF (2 years)"), x = "Lag (weeks)") + acf_theme

  list(acf_st = acf_st, pacf_st = pacf_st, acf_lt = acf_lt, pacf_lt = pacf_lt)
}

# Use your transformed training series here:
pp <- make_acf_pacf_set(train_pp_bc_ts, "Pre-Pandemic")
fh <- make_acf_pacf_set(train_fh_bc_ts, "Full History")

acf_pacf_grid <-
  (pp$acf_st  | fh$acf_st) /
  (pp$pacf_st | fh$pacf_st) /
  (pp$acf_lt  | fh$acf_lt) /
  (pp$pacf_lt | fh$pacf_lt) +
  plot_annotation(
    title = "ACF/PACF diagnostics on transformed training sets",
    subtitle = "Short-term (≤26 weeks) and long-term (≤104 weeks) structure; annual seasonality highlighted near lag 52",
    caption = "Shading marks the annual seasonal cycle region (weeks 50-54)."
  )

acf_pacf_grid
```

## Pre-Modeling Checks
```{r}
#| label: premodel_checks

fmt_num <- function(x, digits = 3) {
  if (is.na(x)) return(NA_character_)
  formatC(x, format = "f", digits = digits)
}

fmt_p <- function(p) {
  if (is.na(p)) return(NA_character_)
  if (p < 0.001) "< 0.001" else fmt_num(p, 3)
}

premodel_checks <- function(x_ts, label) {
  x_ts <- as.ts(x_ts)

  # Unit root / differencing helpers
  adf_p  <- tryCatch(adf.test(x_ts, alternative = "stationary")$p.value, error = \(e) NA_real_)
  kpss_p <- tryCatch(kpss.test(x_ts, null = "Level")$p.value, error = \(e) NA_real_)

  nd_kpss <- tryCatch(ndiffs(x_ts, test = "kpss"), error = \(e) NA_integer_)
  nd_adf  <- tryCatch(ndiffs(x_ts, test = "adf"),  error = \(e) NA_integer_)
  ns_ocsb <- tryCatch(nsdiffs(x_ts, test = "ocsb"), error = \(e) NA_integer_)
  ns_seas <- tryCatch(nsdiffs(x_ts, test = "seas"), error = \(e) NA_integer_)

  tibble(
    dataset = label,
    metric = c(
      "Observations (n)",
      "Start",
      "End",
      "Suggested seasonal diffs D (OCSB)",
      "Suggested seasonal diffs D (SEAS)",
      "Suggested nonseasonal diffs d (KPSS)",
      "Suggested nonseasonal diffs d (ADF)",
      "ADF p-value (H0: unit root)",
      "KPSS p-value (H0: level stationary)"
    ),
    value = c(
      length(x_ts),
      paste(start(x_ts), collapse = ", "),
      paste(end(x_ts), collapse = ", "),
      ns_ocsb,
      ns_seas,
      nd_kpss,
      nd_adf,
      fmt_p(adf_p),
      fmt_p(kpss_p)
    )
  )
}

# ---- build a vertical-friendly table object (rows = metrics) ----
premodel_long <- bind_rows(
  premodel_checks(train_pp_bc_ts, "Pre-Pandemic (PP)"),
  premodel_checks(train_fh_bc_ts, "Full History (FH)")
)

# Option A: keep it fully vertical (never runs off page)
premodel_wide <- premodel_long |>
  pivot_wider(names_from = dataset, values_from = value) |>
  rename(`Pre-modeling check` = metric)

premodel_wide
```

Differencing diagnostics gave mixed recommendations, especially for the Full History series. We therefore treat differencing orders as candidate hyperparameters and select among them using out-of-sample forecast performance on the held-out season, alongside residual diagnostics.

# Model Building

```{r}
#| label: helper_eval-metrics

score_ts_model <- function(fit, dataset, tag , biasadj = TRUE) {
  s <- get_series(dataset)
  
  fc <- forecast(fit, h = length(s$test), biasadj = biasadj)
  
  acc <- accuracy(fc, s$test)
  
  # Enter default (S)ARIMA parameters in case using HW model
  p <- NA_real_
  d <- NA_real_
  q <- NA_real_
  P <- NA_real_
  D <- NA_real_
  Q <- NA_real_
  AICc <- NA_real_
  
  if (inherits(fit, "Arima")) {
    arma <- fit$arma
    p <- arma[1]
    q <- arma[2]
    P <- arma[3]
    Q <- arma[4]
    d <- arma[6]
    D <- arma[7]
    AICc <- fit$aicc
  }
  
  tibble(
    tag = tag, dataset = dataset,
    p = p, d = d, q = q,
    P = P, D = D, Q = Q,
    AICc = AICc,
    RMSE_train = unname(acc["Training set", "RMSE"]),
    MAE_train  = unname(acc["Training set", "MAE"]),
    RMSE_test  = unname(acc["Test set",   "RMSE"]),
    MAE_test   = unname(acc["Test set",   "MAE"])
  )
}
```

## Seasonal Naive Model
```{r}
#| label: model_seas-naive

# Helper to get correct train/test series and lambda
get_series <- function(dataset) {
  if (dataset == "pp") {
    list(train = train_pp_ts, test = test_pp_ts, lambda = lambda_pp)
  } else {
    list(train = train_fh_ts, test = test_fh_ts, lambda = lambda_fh)
  }
}

fit_snaive <- function(dataset) {
  s <- get_series(dataset)
  
  Arima(
    s$train,
    order = c(0, 0, 0),
    seasonal = c(0, 1, 0),
    lambda = s$lambda,
    include.constant = FALSE
  )
}

fit_pp_snaive <- fit_snaive("pp")
fit_fh_snaive <- fit_snaive("fh")
```

```{r}
#| label: metrics_snaive

metrics_pp_snaive <- score_ts_model(fit_pp_snaive, "pp", "snaive_pp")
metrics_fh_snaive <- score_ts_model(fit_fh_snaive, "fh", "snaive_fh")

metrics_pp_snaive
metrics_fh_snaive
```


## Holt-Winters Model
```{r}
#| label: model_holt-winters

fit_hw <- function(dataset) {
  s <- get_series(dataset)
  
  HoltWinters(
    x = s$train,
    seasonal = "additive"
  )
}

fit_pp_hw <- fit_hw("pp")
fit_fh_hw <- fit_hw("fh")
```

```{r}
#| label: metrics_hw

metrics_pp_hw <- score_ts_model(fit_pp_hw, "pp", "hw_pp")
metrics_fh_hw <- score_ts_model(fit_fh_hw, "fh", "hw_fh")

metrics_pp_hw
metrics_fh_hw
```



## SARIMA
```{r}
#| label: model_search

# Define the models to run
cfg <- tribble(
  ~dataset, ~d, ~D, ~tag,
  "pp",      0,  0, "pp_d0_D0",
  "pp",      0,  1, "pp_d0_D1",
  "fh",      0,  0, "fh_d0_D0",
  "fh",      0,  1, "fh_d0_D1",
  "fh",      1,  0, "fh_d1_D0",
  "fh",      1,  1, "fh_d1_D1",
  "pp",     NA, NA, "pp_auto",
  "fh",     NA, NA, "fh_auto"
)


# Fit SARIMA model for a given model
fit_model <- function(dataset, d, D, tag) {
  s <- get_series(dataset)

  if (is.na(d) || is.na(D)) {
    fit <- auto.arima(
      s$train,
      seasonal     = TRUE,
      lambda       = s$lambda,
      biasadj      = TRUE,
      stepwise     = FALSE,
      approximation = FALSE,
      max.p = 3, max.q = 3, max.P = 2, max.Q = 2
    )
  } else {
    fit <- auto.arima(
      s$train,
      d = d, D = D,
      seasonal     = TRUE,
      lambda       = s$lambda,
      biasadj      = TRUE,
      stepwise     = FALSE,
      approximation = FALSE,
      max.p = 3, max.q = 3, max.P = 2, max.Q = 2
    )
  }

  fit
}

# Prevents one model having issues from making the whole time spent running models wasted
safe_fit_model <- safely(fit_model, otherwise = NULL, quiet = TRUE)

# Fit all models from `cfg` in parallel and cache the models for later use
model_search_out <- cache_computation({
  
  out <- cfg |>
    mutate(
      res = future_pmap(
        .l = list(dataset, d, D, tag),
        .f = safe_fit_model,
        .options = furrr_options(seed = TRUE)
      )
    ) |>
    mutate(
      fit = map(res, "result"),
      err = map(res, "error")
    )
  
  # Keep successful model fits
  models_tbl <- out |>
    filter(map_lgl(err, is.null)) |>
    transmute(
      tag, dataset, d, D, fit
    )

    # Document failed models + error messages
  errors_tbl <- out |>
    filter(!map_lgl(err, is.null)) |>
    transmute(
      tag, dataset, d, D,
      error = map_chr(err, ~ conditionMessage(.x))
    )

  # What gets cached
  list(
    models_tbl = models_tbl,
    errors_tbl = errors_tbl
  )
},
cache_filename = "arima_model_search_models_cache.rds",
deps = list(
  cfg = cfg,
  lambda_pp = lambda_pp,
  lambda_fh = lambda_fh,
  pp_start = start(train_pp_ts), pp_end = end(train_pp_ts),
  fh_start = start(train_fh_ts), fh_end = end(train_fh_ts),
  pp_test_start = start(test_pp_ts), pp_test_end = end(test_pp_ts),
  fh_test_start = start(test_fh_ts), fh_test_end = end(test_fh_ts)
),
parallel = TRUE)

models_tbl <- model_search_out$models_tbl
errors_tbl <- model_search_out$errors_tbl

models_tbl
errors_tbl
```

```{r}
#| label: metrics_sarima

sarima_metrics_tbl <- models_tbl |> 
  mutate(
    # Compute metrics for each SARIMA model candidate
    metrics = pmap(
      list(
        fit = fit,
        dataset = dataset,
        tag = tag
      ),
      ~ score_ts_model(
        fit = ..1,
        dataset = ..2,
        tag = ..3
      )
    )
  ) |> 
  transmute(metrics) |> 
  unnest(cols = metrics) |> 
  arrange(dataset, AICc)

sarima_metrics_tbl |> 
  select(-tag) |> 
  kable(digits = 3, booktabs = TRUE) |> 
  add_header_above(c(" " = 1, "Model parameters" = 6, "In-Sample Accuracy" = 3, "Out-of-Sample Accuracy" = 2)) |> 
  kable_styling(bootstrap_options = "striped") |> 
  row_spec(c(1, 6), background = nav_cols[5])

# Store best SARIMA models and table rows
fit_fh_sarima <- models_tbl |> 
  filter(dataset == "fh", d == 0, D == 0) |> 
  pull(fit)
fit_fh_sarima <- fit_fh_sarima[[1]]

metrics_fh_sarima <- score_ts_model(
  fit = fit_fh_sarima,
  dataset = "fh",
  tag = "fh_sarima"
)

fit_pp_sarima <- models_tbl |> 
  filter(dataset == "pp", d == 0, D == 0) |> 
  pull(fit)
fit_pp_sarima <- fit_pp_sarima[[1]]

metrics_pp_sarima <- score_ts_model(
  fit = fit_pp_sarima,
  dataset = "pp",
  tag = "pp_sarima"
)
```

Rough SARIMA model breakdown
- Models look at last two weeks' ILI levels (p = 2)
  - $t-1, t-2$
- Models look at last two years of seasonal data for the same week (P = 2)
  - $t-52, t-104$
- Models look at noise from previous week
  - $t-1$

```{r}
#| label: compare_models

all_metrics <- bind_rows(
  metrics_pp_snaive,
  metrics_pp_hw,
  metrics_pp_sarima,
  metrics_fh_snaive,
  metrics_fh_hw,
  metrics_fh_sarima
)

all_metrics |> 
  select(-tag) |>
  kable(digits = 3, booktabs = TRUE) |> 
  add_header_above(c(" " = 1, "Model parameters" = 6, "In-Sample Accuracy" = 3, "Out-of-Sample Accuracy" = 2)) |>
  kable_styling(bootstrap_options = "striped")
```

# Model Prediction
## Cumulative Mean Average Error

Due to the possibility of the seasonally naive model getting lucky with a really good estimate right off the bat, I'll decide a priori a minimum number of weeks to include in the CMAE calculation ($h_{min}$) to evaluate before considering if the more sophisticated models actually perform better than the seasonal naive model. To be objective about this, I'm going to define $h_{min}$ before running any analysis, and base it off of typical staffing timelines and supply ordering timelines that might be present in a medical office. Most skilled medical staff in the US work in situations where the schedule must be posted 4-6 weeks in advance [@wangEvaluatingImpactFlexibility2009]. I couldn't find anything concrete on timelines of requesting travel nurses and them arriving, however travel nurse contracts usually vary from about 2-26 weeks in length [@rnHowLongAre2025], and nurses planning on working as travel nurses are given the advice that it usually takes 1-5 weeks between beginning to apply for traveling nursing jobs and actually starting at them [@schmidtHowLongDoes2018]. For the time it takes between ordering and receiving supplies, typically this is very low, however in times of increased demand (e.g., the 2009 H1N1 flu season), delivery times for PPE were increased in the range of 2 to 10 weeks [@patelPersonalProtectiveEquipment2017]. Taking all of these together, I will set the minimum horizon for evaluating CMAE to $h_{min} = 4$ weeks. That is, I will only consider the more complex models to have a meaningful advantage over the seasonal naive model if their CMAE is lower from at least week 4 onwards.

```{r}
#| label: helper_cmae

# Helper to compute CMAE(h) for a model
compute_cmae <- function(fit, dataset, tag, h_max = 52, biasadj = TRUE) {
  s <- get_series(dataset)
  
  H <- min(h_max, length(s$test))
  
  y_true <- as.numeric(s$test)[seq_len(H)]
  
  if (inherits(fit, "forecast")) {
    y_pred <- as.numeric(fit$mean)[seq_len(H)]
  } else {
    fc <- forecast(fit, h = H, biasadj = biasadj)
    y_pred <- as.numeric(fc$mean)
  }
  
  abs_err <- abs(y_true - y_pred)
  
  tibble(
    tag = tag,
    dataset = dataset,
    h = seq_len(H),
    abs_err = abs_err,
    CMAE = cumsum(abs_err) / h
  )
}

# Helper to compute h* for a model vs seasonal naive baseline
eff_horiz <- function(cmae_model, cmae_snaive, h_min = 4L) {
  joined <- inner_join(
    cmae_model  |> select(dataset, h, CMAE_model  = CMAE),
    cmae_snaive |> select(dataset, h, CMAE_snaive = CMAE),
    by = c("dataset", "h")
  )
  
  ok <- joined |>
    filter(
      h >= h_min,
      CMAE_model <= CMAE_snaive
    )
  
  if (nrow(ok) == 0L) {
    return(0L)
  }
  
  max(ok$h)
}

# Helper to create plots
make_fc_df <- function(fit, dataset, tag, h_max = 52,
                       level = 95, biasadj = TRUE) {
  s <- get_series(dataset)
  
  # forecast horizon = min(h_max, length of test set)
  H <- min(h_max, length(s$test))
  
  fc <- forecast(
    fit,
    h       = H,
    level   = level,
    biasadj = biasadj
  )
  
  tibble(
    dataset     = dataset,
    tag         = tag,
    season_week = seq_len(H),
    y_true      = as.numeric(s$test)[seq_len(H)],
    mean        = as.numeric(fc$mean),
    lo          = as.numeric(fc$lower[, 1]),
    hi          = as.numeric(fc$upper[, 1])
  )
}
```

```{r}
#| label: compute-cmae_all-models
 
# Seasonal Naive Baseline
cmae_pp_snaive <- compute_cmae(fit_pp_snaive, "pp", "pp_snaive", h_max = 52)
cmae_fh_snaive <- compute_cmae(fit_fh_snaive, "fh", "fh_snaive", h_max = 52)

# Holt-Winters Comparison
cmae_pp_hw <- compute_cmae(fit_pp_hw, "pp", "pp_hw", h_max = 52, biasadj = FALSE)
cmae_fh_hw <- compute_cmae(fit_fh_hw, "fh", "fh_hw", h_max = 52, biasadj = FALSE)

# SARIMA Models
cmae_pp_sarima <- compute_cmae(fit_pp_sarima, "pp", "pp_sarima", h_max = 52)
cmae_fh_sarima <- compute_cmae(fit_fh_sarima, "fh", "fh_sarima", h_max = 52)

# Combine into one object
cmae_all <- bind_rows(
  cmae_pp_snaive, cmae_fh_snaive,
  cmae_pp_hw, cmae_fh_hw,
  cmae_pp_sarima, cmae_fh_sarima
)
```

```{r}
#| label: compute_eff-horiz

h_min <- 4L

# Pre-pandemic dataset
h_star_pp_sarima <- eff_horiz(cmae_pp_sarima, cmae_pp_snaive, h_min = h_min)
h_star_pp_hw <- eff_horiz(cmae_pp_hw, cmae_pp_snaive, h_min = h_min)

# Full history dataset
h_star_fh_sarima <- eff_horiz(cmae_fh_sarima, cmae_fh_snaive, h_min = h_min)
h_star_fh_hw <- eff_horiz(cmae_fh_hw, cmae_fh_snaive, h_min = h_min)

eff_horiz_tbl <- tibble(
  dataset = c("pp", "pp", "fh", "fh"),
  tag = c("pp_sarima", "pp_hw", "fh_sarima", "fh_hw"),
  h_min = h_min,
  h_star = c(h_star_pp_sarima, h_star_pp_hw,
             h_star_fh_sarima, h_star_fh_hw)
)

eff_horiz_tbl
```

```{r}
#| label: fc_tidy_all

# Pre-pandemic (pp)
pp_fc_all <- bind_rows(
  make_fc_df(fit_pp_snaive, "pp", "snaive", h_max = 52),
  make_fc_df(fit_pp_hw,     "pp", "HW",     h_max = 52, biasadj = FALSE),
  make_fc_df(fit_pp_sarima, "pp", "SARIMA", h_max = 52)
)

# Full history (fh)
fh_fc_all <- bind_rows(
  make_fc_df(fit_fh_snaive, "fh", "snaive", h_max = 52),
  make_fc_df(fit_fh_hw,     "fh", "HW",     h_max = 52, biasadj = FALSE),
  make_fc_df(fit_fh_sarima, "fh", "SARIMA", h_max = 52)
)

# Distinct test observations for each dataset
pp_obs <- pp_fc_all |> distinct(season_week, y_true)
fh_obs <- fh_fc_all |> distinct(season_week, y_true)

label_models <- c(
  snaive = "Seasonal naive",
  HW     = "Holt-Winters",
  SARIMA = "SARIMA"
)

pp_fc_all <- pp_fc_all |>
  mutate(tag = factor(tag, levels = c("snaive", "HW", "SARIMA"),
                      labels = label_models))

fh_fc_all <- fh_fc_all |>
  mutate(tag = factor(tag, levels = c("snaive", "HW", "SARIMA"),
                      labels = label_models))
```

```{r}
#| label: cmae_tidy_all

cmae_all <- bind_rows(
  cmae_pp_snaive,
  cmae_pp_hw,
  cmae_pp_sarima,
  cmae_fh_snaive,
  cmae_fh_hw,
  cmae_fh_sarima
)

# Clean up model labels (same mapping as before)
cmae_all <- cmae_all |>
  mutate(
    model = case_when(
      grepl("snaive", tag)  ~ "Seasonal naive",
      grepl("hw",     tag)  ~ "Holt-Winters",
      grepl("sarima", tag)  ~ "SARIMA",
      TRUE                  ~ tag
    ),
    model = factor(model, levels = c("Seasonal naive", "Holt-Winters", "SARIMA"))
  )

pp_cmae_all <- cmae_all |> filter(dataset == "pp")
fh_cmae_all <- cmae_all |> filter(dataset == "fh")

# Shared legend order
model_levels <- c("Observed", "Seasonal naive", "Holt-Winters", "SARIMA")

# Shared colors
model_cols <- c(
  "Observed"       = "black",
  "Seasonal naive" = nav_cols[4],
  "Holt-Winters"   = nav_cols[2],
  "SARIMA"         = nav_cols[1]
)

# Shared linetypes
model_lty <- c(
  "Observed"       = "solid",
  "Seasonal naive" = "solid",
  "Holt-Winters"   = "dotted",
  "SARIMA"         = "dashed"
)

# Shared line widths
model_lw <- c(
  "Observed"       = 1.2,
  "Seasonal naive" = 0.9,
  "Holt-Winters"   = 0.9,
  "SARIMA"         = 0.9
)
```

```{r}
#| label: plot_pp_test_fc

gg_pp_test <- ggplot() +
  # True test series (Observed)
  geom_line(
    data = pp_obs,
    aes(x = season_week, y = y_true,
        colour   = "Observed",
        linetype = "Observed",
        linewidth = "Observed")
  ) +
  # Forecast intervals (HW + SARIMA only)
  geom_ribbon(
    data = pp_fc_all |> dplyr::filter(tag != "Seasonal naive"),
    aes(x = season_week, ymin = lo, ymax = hi, fill = tag),
    alpha = 0.15
  ) +
  # Forecast means
  geom_line(
    data = pp_fc_all,
    aes(x = season_week, y = mean,
        colour   = tag,
        linetype = tag,
        linewidth = tag)
  ) +
  scale_colour_manual(
    values = model_cols,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_fill_manual(
    values = model_cols,
    breaks = model_levels[-1],   # no fill for Observed
    drop   = FALSE
  ) +
  scale_linetype_manual(
    values = model_lty,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linewidth_manual(
    values = model_lw,
    breaks = model_levels,
    drop   = FALSE
  ) +
  labs(
    title    = "Pre-pandemic test period",
    x        = "Week into flu season",
    y        = "Weighted ILI (%)",
    colour   = "Model",
    fill     = "Model",
    linetype = "Model",
    linewidth = "Model"
  ) +
  theme_minimal()
```

```{r}
gg_fh_test <- ggplot() +
  # Observed full-history test season
  geom_line(
    data = fh_obs,
    aes(x = season_week, y = y_true,
        colour   = "Observed",
        linetype = "Observed",
        linewidth = "Observed")
  ) +
  # Forecast intervals (HW + SARIMA only)
  geom_ribbon(
    data = fh_fc_all |> dplyr::filter(tag != "Seasonal naive"),
    aes(x = season_week, ymin = lo, ymax = hi, fill = tag),
    alpha = 0.15
  ) +
  # Forecast means
  geom_line(
    data = fh_fc_all,
    aes(x = season_week, y = mean,
        colour   = tag,
        linetype = tag,
        linewidth = tag)
  ) +
  scale_colour_manual(
    values = model_cols,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_fill_manual(
    values = model_cols,
    breaks = model_levels[-1],  # no fill for Observed
    drop   = FALSE
  ) +
  scale_linetype_manual(
    values = model_lty,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linewidth_manual(
    values = model_lw,
    breaks = model_levels,
    drop   = FALSE
  ) +
  labs(
    title    = "Full-history test period",
    x        = "Week into flu season",
    y        = "Weighted ILI (%)",
    colour   = "Model",
    fill     = "Model",
    linetype = "Model",
    linewidth = "Model"
  ) +
  theme_minimal()
```

```{r}
# Map tags -> model labels like you did for cmae_all
eff_marks <- eff_horiz_tbl |>
  mutate(
    model = case_when(
      grepl("sarima", tag) ~ "SARIMA",
      grepl("hw",     tag) ~ "Holt-Winters",
      TRUE                 ~ NA_character_
    )
  ) |>
  filter(!is.na(model), h_star > 0L)

# Join to get CMAE(h*) for each dataset/model
eff_marks_plot <- eff_marks |>
  left_join(
    cmae_all,
    by = c("dataset", "model" = "model", "h_star" = "h")
  ) |>
  rename(CMAE_star = CMAE)
```

```{r}
gg_pp_cmae <- ggplot(pp_cmae_all,
                     aes(x = h, y = CMAE,
                         colour   = model,
                         linetype = model,
                         linewidth = model)) +
  geom_line() +
  geom_vline(xintercept = h_min, linetype = "dotted", colour = "grey40") +
  geom_point(
    data = eff_marks_plot |> dplyr::filter(dataset == "pp"),
    aes(x = h_star, y = CMAE_star, colour = model),
    size = 2
  ) +
  scale_colour_manual(
    values = model_cols,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linetype_manual(
    values = model_lty,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linewidth_manual(
    values = model_lw,
    breaks = model_levels,
    drop   = FALSE
  ) +
  labs(
    title = "Pre-pandemic CMAE by forecast horizon",
    x     = "Forecast horizon h (weeks ahead)",
    y     = "CMAE(h), Weighted ILI (%)",
    colour   = "Model",
    linetype = "Model",
    linewidth = "Model"
  ) +
  annotate(
    "text",
    x = h_min + 0.5, y = Inf,
    label = sprintf("h[min] == %d", h_min),
    parse = TRUE,
    vjust = 1.5, hjust = 0, size = 3
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
gg_fh_cmae <- ggplot(
  fh_cmae_all,
  aes(x = h, y = CMAE,
      colour   = model,
      linetype = model,
      linewidth = model)
) +
  geom_line() +
  geom_vline(xintercept = h_min, linetype = "dotted", colour = "grey40") +
  geom_point(
    data = eff_marks_plot |> dplyr::filter(dataset == "fh"),
    aes(x = h_star, y = CMAE_star, colour = model),
    size = 2
  ) +
  scale_colour_manual(
    values = model_cols,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linetype_manual(
    values = model_lty,
    breaks = model_levels,
    drop   = FALSE
  ) +
  scale_linewidth_manual(
    values = model_lw,
    breaks = model_levels,
    drop   = FALSE
  ) +
  labs(
    title = "Full-history CMAE by forecast horizon",
    x     = "Forecast horizon h (weeks ahead)",
    y     = "CMAE(h), Weighted ILI (%)",
    colour   = "Model",
    linetype = "Model",
    linewidth = "Model"
  ) +
  annotate(
    "text",
    x = h_min + 0.5, y = Inf,
    label = sprintf("h[min] == %d", h_min),
    parse = TRUE,
    vjust = 1.5, hjust = 0, size = 3
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
forecast_cmae_grid <-
  (gg_pp_test | gg_pp_cmae) /
  (gg_fh_test | gg_fh_cmae) +
  plot_annotation(
    title = "Forecast performance by model and dataset",
    subtitle = "Left: one-season-ahead forecasts over the test season.\nRight: cumulative mean absolute error as horizon increases."
  ) +
  plot_layout(guides = "collect") &
  theme(legend.position = "right")

forecast_cmae_grid
```




# Figures and Tables
## Data Cleaning
### Raw Data Plot
```{r}
#| label: raw_data_plot
#| eval: false
#| include: false

raw_data_plot <- plot(ili_ts_raw, 
     main = "Weighted ILI Activity", 
     ylab = "Weighted ILI %")

raw_data_plot

save_plot(raw_data_plot, "raw_data", format = "png")
```

### Clean Data Plot
```{r}
#| label: clean_data_plot
#| eval: false
#| include: false

clean_data_plot <- plot(ili_ts, 
     main = "Weighted ILI Activity (Cleaned)", 
     ylab = "Weighted ILI %")

clean_data_plot

save_plot(clean_data_plot, "clean_data", format = "png")
```

## Exploratory Analysis
### Flu Overview Plot
```{r}
#| label: flu_overview_plot
#| eval: false
#| include: false

flu_overview
save_plot(flu_overview, "flu_overview")
```

### Peak Variability Table
```{r}
#| label: peak_variability_table
#| eval: false
#| include: false

# How to import into paper and format

# peak_compare_tbl <- readRDS(here("output", "tables", "peak_compare_tbl.rds"))
#
# peak_compare_tbl |>
#   kable(
#     format = "latex",
#     booktabs = TRUE,
#     align = c("l", "c", "c", "c", "c"), # Center align data columns
#     caption = "Season-level peak timing and intensity summaries."
#   ) |>
#   # CHANGE 2: Add a spanning header to explicitly link the test to the groups
#   add_header_above(c(
#     " " = 1, 
#     "Cohort Comparison (Welch t-test)" = 3, 
#     "Reference" = 1
#   )) |>
#   kable_styling(latex_options = c("hold_position", "scale_down"))

saveRDS(peak_compare_tbl, here("output", "tables", "peak_compare_tbl.rds"))
```

### Peak Variability Plot
```{r}
#| label: peak_variability_plot
#| eval: false
#| include: false

peak_var_plot
save_plot(peak_var_plot, "peak_variability")
```

## Analysis Prep
### Data Transformation Plot
```{r}
#| label: data_transformation_plot
#| eval: false
#| include: false

data_transformation_plot
save_plot(data_transformation_plot, "data_transformation")
```

### Decomposition Plot
```{r}
#| label: decomposition_plot
#| eval: false
#| include: false

decomp_plot
save_plot(decomp_plot, "decomposition")
```


### ACF-PACF Plot
```{r}
#| label: acf-pacf_plot
#| eval: false
#| include: false

acf_pacf_grid
save_plot(acf_pacf_grid, "acf_pacf")
```

### Pre-Modeling Checks Table
```{r}
#| label: premodeling_checks_table
#| eval: false
#| include: false

# How to import into paper and format

# premodel_tbl <- readRDS(here("output", "tables", "premodel_checks_tbl.rds"))
# 
# premodel_tbl |>
#   kable(format = "latex", booktabs = TRUE,
#         caption = "Pre-modeling checks on Box-Cox transformed training sets.") |>
#   kable_styling(latex_options = "hold_position")

premodel_wide
saveRDS(premodel_wide, here("output", "tables", "premodel_wide_tbl.rds"))
```



## Model Building
```{r}
#| label: model_eval_tables
#| eval: false
#| include: false

# SARIMA model selection table w/ models
saveRDS(results_tbl, here("output", "tables", "arima_model_selection_tbl.rds"))
if (nrow(errors_tbl) > 0) {
  saveRDS(errors_tbl, here("output", "tables", "model_selection_errors_tbl.rds"))
}

# All model comparison table
saveRDS(all_metrics, here("output", "tables", "model_comparison_tbl.rds"))
```

