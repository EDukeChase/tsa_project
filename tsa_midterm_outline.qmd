---
title: "Timerseries Analysis Midterm Outline"
author: "E. Duke Chase"
date: today
---

```{r}
#| label: load-packages
#| echo: false

library(here)
library(kableExtra)
```


# Outline
## Question
(a) State your research/scientific/policy question and how answering it translates to real world decisions

\color{blue}
How far out can we forecast weekly flu-like illness occurrence with acceptable accuracy? Has this horizon changed significantly since COVID-19? This question could be useful in real world decisions because it could be used in healthcare environments for predicting upcoming needs due to flu season, like staffing, stocking vaccinations, medication supplies, etc.
\color{black}

(b) Describe how time series will be used to answer it. For example: prediction, trend modeling, detection of anomalies, etc.

\color{blue}
For each forecast horizon $h \in \{1,2,\ldots,52\}$, I will compute cumulative mean absolute error: $$\mathrm{CMAE}(h) = \frac{1}{h}\sum_{k=1}^{h}\left|y_{t+k}-\hat{y}_{t+k\mid t}\right|,$$
where $t$ is the final week in the training set and $\hat{y}_{t+k\mid t}$ is the $k$-step-ahead forecast for the model under consideration.

I define the effective forecast horizon as$$h^* = \max\{h : h \geq h_{min} \land \mathrm{CMAE}_{model}(h) \leq \mathrm{CMAE}_{\text{snaive}}(h)\},$$ where $\mathrm{CMAE}_{\text{snaive}}(h)$ is computed analogously for a seasonal naive baseline. If the SARIMA never beats the seasonally naive model, I'll denote that with $h* = 0$
\color{black}

## Data
(a) Justify (and/or provide limitations in) how your chosen data is the correct data for your question

\color{blue}
The data is a good choice because it looks at flu-like illness prevalence throughout the country, but this does come with some limitations. It is measuring the percentage of patients that come in to doctors with influenza-like illnesses, and while I tend to generally consider relative measurements best, if for some reason some other disease spiked at the same time as ILIs did, but to a greater extent than ILIs, that would make the ILI measure we have go down, even though the total number of cases went up. Another issue with the data is that ILI is defined as "fever (temperature of 100°F [37.8°C] or greater) and a cough and/or a sore throat," [@ili_methods] so any increase in the prevalence of diseases with those symptoms can result in a bump. There is alternate data available that come from testing laboratories and are based on confirmed influenza cases, but there's a selection bias there since not all ILI result in a sample being sent to a lab to be cultured. An additional limitation is that generally only more severe cases of ILI will result in doctor's visits, but there's not really much to be done about that other than to document it.
\color{black}

(b) Discuss any data processing/cleaning/preparation steps and/or decisions related to the identification/creation of the data you used in your analysis

\color{blue}    
I'll have to get rid of extraneous variables that won't be used in my data, such as region (not relevant because I'm looking at the national level), or age makeup of patients. The other big issue I'll have to deal with is what to do with years that have 53 weeks instead of 52; my plan at the moment is to naively drop week 53, run my analysis, and then I can do a sensitivity analysis comparing other methods of dealing with week 53 once I have an outcome I can judge the sensitivity analysis on. Other than that, the data are pretty clean already, and I shouldn't have to create any variables to analyze.

As a note: 5 of the 28 years of data had 53 weeks, with the final reduced dataset only have 3 years with 53rd weeks that were removed.
\color{black}

(c) Provide a source for your data

\color{blue}
The data comes from the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), and was retrieved from the CDC's FluView portal [@cdc_fluview].
\color{black}

(d) Describe key features/properties of your time series such as size, time step units, variable being studied, etc.

\color{blue}
The full dataset includes 1,467 weekly measurements spanning the time period of 1997 - 2025, although after removing a significant amount due to incompleteness or significant methodological change, the data set used for this analysis consists of 1,040 weekly measurements spanning 2005-2024. The variable being studied is percentage of doctor's office visits that are related to influenza-like illnesses.
\color{black}

## Statistical Plan
(a) Provide a brief a priori analytical plan for your project. What pre-processing, visualizing, modeling steps, etc. will you conduct? (Note: what you do for item d. in the Data component section above should be described here as well)

\color{blue}
Start out with basic data cleaning and validation, ensuring that the data I have are usable for the analysis. Then I'll create basic plots of ILI percentage over time, find basic descriptive statistics such as when seasons normally peak and the variability of that. Then I'll split the data into pre-pandemic (PP) and full history (FH) datasets and see if I can transform the data in some way to try to account for the significant variability that is present in flu seasons due to their biological nature. Next I'll visualize decompositions of the PP and FH data and consider if I might need to make further transformations based on the remainder after trend and seasonality are accounted for. Once I feel confident with that, I'll plot the ACF and PACF, perform ADF and KPSS tests for stationarity, and try to use those to inform the parameters for my SARIMA model. I will also estimate a Holt-Winters model to serve as a secondary comparison for the SARIMA model, and use CMAE to compare them to each other and to the baseline seasonally naive model to assess performance.
\color{black}

(b) Explain how the steps in your plan build a framework for making a robust conclusion for your research question. That is, how does your plan give a complete answer to your question?

\color{blue}
My framework is designed to translate forecasting performance into an operationally meaningful “how many weeks ahead can we trust this?” answer. I use one-season-ahead evaluation in two regimes: a pre-pandemic control (train on all PP seasons except 2018–2019; test on 2018–2019) and a full-history regime that includes pandemic disruptions (train on all FH seasons except 2024–2025; test on 2024–2025). For each model, I evaluate accuracy not only with overall MAE/RMSE on the test season, but also with cumulative mean absolute error, CMAE(h), which reveals how forecast errors accumulate as the horizon grows. I then define an effective horizon $h*$ as the largest horizon $h \geq 4$ where CMAE for the model is no worse than the seasonal naïve baseline. This directly answers the policy question by identifying how far ahead a clinic could plan (staffing, supplies) while still outperforming a simple “repeat the same week from last year” strategy, and whether that planning window appears to change after COVID-era disruptions.
\color{black}

## Exploratory Data Analysis
(a) Provide any summary or other statistics/details relevant to your analytical plan and those needed to understand your final conclusions. E.g. any descriptive statistics of your time series or any feature (such as a pattern or trend) of your time series

```{r}
#| label: descriptive-plots
#| echo: false

peak_compare_tbl <- readRDS(here("output", "tables", "peak_compare_tbl.rds"))

peak_compare_tbl |>
  kable(
    format = "latex",
    booktabs = TRUE,
    align = c("l", "c", "c", "c", "c"), # Center align data columns
    caption = "Season-level peak timing and intensity summaries."
  ) |>
  # CHANGE 2: Add a spanning header to explicitly link the test to the groups
  add_header_above(c(
    " " = 1,
    "Cohort Comparison (Welch t-test)" = 3,
    "Reference" = 1
  )) |>
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

\color{blue}
The dataset did not include any data during the flu off-season (weeks 21-39 of the year) until 2003, when a relatively small prortion of reporters submitted some data during the off-season [@CDCExpandMonitoring] (this is possibly related to SARS being discovered around this time, but I couldn't find a definitive source attesting to that). In 2004, ILI monitoring was expanded to include collecting data on children during the off-season months, and finally, after the 2005-2006 flu season, it became the standard practice to collect data for both children and adults year round [@CDCSum03]. Data included 5 years with 53 weeks instead of 52, so the 53rd weeks were dropped from the analysis (somewhat common solution to the problem based on what I've read), which obviously biases the data and introduces some mismatch in terms of year-over-year alignment. For the analysis I began the series at the onset of the 2005-06 flu season (week 40 of 2005) due to the change in data collection methodology, and modeled it as a weekly time series with a frequency of 52 weeks. There is strong annual seasonality with a relatively consistent trend of peaking in severity around week 15 of the flu season (week 3 of the year) although there is substantial variability in the timing and intensity of the peaks. There is a much more consistent trend of falling off to very low levels around 1-2% around week 30 of the flu season (18 weeks into the year). Several seasons show significant abnormalities in either intensity or timing of peak flu load.

Stationarity diagnostics are somewhat mixed, with an ADF test suggesting stationarity (p = 0.01), a KPSS test for stationarity with trend failing to reject stationarity (p = 0.10), while a KPSS test for level stationarity provided evidence against stationarity (p = 0.01). Visual inspection reveals a strong seasonality to the data, and variable amplitude in peaks suggests transforming the data to try and minimize the effect of this variance on the analysis before beginning to look at decomposition and ARIMA diagnostics.
\color{black}

(b) Provide and discuss at least one exploratory visualization of your time series. For example, how the visualization guided/motivated any decisions about how to use/model the data or provides insights into your question’s answer

\color{blue}
![fluoverview](output/figures/flu_overview.png)
In the main figure we can see that for most flu seasons there's a peak between 13-18 weeks into the season, but there is a reasonable amount of variation in this, with one peak happening as early as 3 weeks into the flu season, and another as late as 25 weeks into the season. This is definitely more variation than would be seen just due to deleting week 53 or the slight variation in how weeks line up year to year. Interestingly, despite the wide distribution of peaks during flu season, the trough of flu season almost universally occurs around week 30.

Around the main plot I've highlighted a few flu seasons that had interesting characteristics. Subplots (a) and (c) were both heavy flu years, with (a) reflecting the H1N1 (swine flu) epidemic; swine flu had a very early start in the year compared to typical flu variants, and is also the only time I've experienced hallucinations due to fever. Subplot (b) was noteworthy due to being an incredibly mild flu season. Subplot (g) was interesting because it had two distinct peaks during the season. Lastly, subplots (d), (e) and (f) are especially indicative of the effects COVID-19 had on flu occurrence. Subplot (d) shows the flu season which included the initial spread of COVID-19, which may explain why it has three peaks instead of one like the majority of seasons have, especially since early COVID-19 strains had significant symptom overlap with influenza, and this data is related to doctor visits, not tested strains. Subplot (e) occurred during the height of non-pharmaceutical interventions intended to slow the spread (masking, social distancing, etc.)[@covid_npi], showing that there was almost no detectable flu season as a result. Subplot (f) occurred soon after the vast majority of state-level interventions had been lifted[@covid_npi], and peaked earlier in the year than typical, possibly lending credence to the idea of immune debt [@immune_debt] causing people to be more vulnerable to infection after a period of low exposure.

![acfpacf](output/figures/acf_pacf_plots.png)
The top shorter-term ACF plot does not exhibit exponential decay, while the short-term PACF shows strong spikes at lags 1 and 2, but then is near 0 with a few random significant spikes which are to be expected just due to the number of lags being evaluated. Looking at the long-term plots, the sinusoidal appearance of the ACF is strongly indicative of a seasonal trend, and the long term PACF plot reinforces this, with lags near week 52 generally being significant.
\color{black}

## Statistical Analysis

(a) Estimate and interpret, in the context of your question, at least one non-ARIMA model that we used in class prior to ARIMA. For example, any of the smoothers or any of the mathematical models such as linear, exponential, polynomial, etc. Justify an related assumptions for these models (if there are any). In the Statistical Plan section you should motivate your choice in the sense of what it lets you learn about your time series related to your project goals

\color{blue}

\color{black}

(b) Estimate an ARIMA model for your time series

    (i) Justify how, and that, you met any assumptions needed for an ARIMA model and explain why they must be met for ARIMA to be valid. Examples include stationarity, stable variance, etc. Examples could also include showing there is informative autocorrelation in the time series

\color{blue}

\color{black}
    (ii) If there was seasonality, justify your SARIMA choices

\color{blue}

\color{black}
    (iii) Justify your approach to choosing p, d, and q, and report each step/phase of this approach when applied to the data. For example, if you will systematically consider multiple models, describe this process and then provide results related to the series of candidate models. Alternatively, if you have a way to choose a single p, d, and q before estimating any models, explain how you will make this choice and why it is likely to lead to a reasonable ARIMA model. There are other approaches such as citing expert opinion or even doing something entirely new (that you can scientifically/statistically justify)

\color{blue}

\color{black}
    (iv) Provide and interpret ACF and PACF plots

\color{blue}

\color{black}
    (v) Justify your metric(s) for choosing a best/final model

\color{blue}

\color{black}
    (vi) Provide and interpret the statistical significance of any AR or MA coefficients in your final estimated model

\color{blue}

\color{black}
    (vii) Analyze the residuals to validate your final ARIMA model. Explain why each check you are performing with the residuals is necessary

\color{blue}

\color{black}

## Final Model and Conclusions
(a) Provide an equation for your final estimated model and explain all parameters and variables in this equation
(b) Use the model and/or its statistical output to answer your research question. Provide both a statistical answer (e.g. the calculated prediction interval was _____) and a practical answer (e.g. implications of your conclusion/answer/inference in the real world domain of your question)
(c) Discuss the strengths and limitations of ARIMA for your data/question. Note: the final project for our class is an extension of this project where you will compare your ARIMA model to other models from other approaches; thus, this could be a reasonable place to start motivating properties you would ideally like in different models

## General Guidelines/Requirements
(a) Provide the above in a logically ordered, professional (polished and error free), and
easily readable format
(b) It is possible that not every project will need every step in every component and/or
the order of components may change. Use your best judgement (and/or ask me) to
include any of the above components (or even ones I didn’t mention) relevant to your
project
(c) Do not just cut and paste R output and add a few sentences around the output
(d) Provide a bibliography section for any sources used. This could be related to the data
or any source you used (except you don’t have to site me) when conducting the
project work
(e) Submit a single pdf version of your report (please double space)
(f) Provide me both your data and a reasonably clean/commented version of your R code

# References
::: {#refs}
:::